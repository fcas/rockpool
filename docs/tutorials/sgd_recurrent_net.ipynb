{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ∇ Gradient descent training of a rate-based recurrent network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This tutorial demonstrates using |project| and a ``Jax``-accelerated rate-based recurrent module to perform gradient descent training of all network parameters. The result is a trained dynamic recurrent network with long memory, optimised to perform a signal generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This example requires the |project| package from SynSense, as well as ``jax`` and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# - Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# - Imports and boilerplate \n",
    "from rockpool import TimeSeries, TSContinuous\n",
    "from rockpool.nn.modules import RateJax, LinearJax\n",
    "from rockpool.nn.combinators import Sequential\n",
    "from rockpool.parameters import Constant\n",
    "from tqdm import tnrange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# - Numpy\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "# - Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# - Pretty printing\n",
    "try:\n",
    "    from rich import print\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggered signal-generation task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will use a pulse-to-chirp task as a demonstration. The recurrent network receives a short pulse, and must respond by generating a chirp signal (a sinusoid increasing in frequency over time). This task is difficult, as no input is present for most of the time, and so considerable temporal memory is required in the network.\n",
    "\n",
    "You can adjust the parameters of the input by changing the number of repeats ``num_repeats``, the duration of the input pulse ``pulse_duration``, and the maximum frequency reached by the chirp ``chirp_freq_factor`` in the cell below. Shorter pulses and higher chirp frequencies make the problem more difficult. More repeats make BPTT learning more difficult, by forcing gradients to be accumulated over more time steps.\n",
    "\n",
    "You can also adjust the time step ``dt``, which makes learning slower (more time points evaluated per trial), but which permits shorter time constants to be used in the network. For numerical stability, time constants must be at least ``10*dt``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# - Define input and target signals \n",
    "num_repeats = 1\n",
    "pulse_duration = 50e-3\n",
    "chirp_freq_factor = 10\n",
    "padding_duration = 1\n",
    "chirp_duration = 1\n",
    "dt = 1e-3\n",
    "\n",
    "# - Build chirp and trigger signals\n",
    "chirp_end = int(np.round(chirp_duration / dt))\n",
    "chirp_timebase = np.linspace(0, chirp_end * dt, chirp_end)\n",
    "chirp = np.atleast_2d(np.sin(chirp_timebase * 2 * np.pi * (chirp_timebase * chirp_freq_factor))).T\n",
    "trigger = np.atleast_2d(chirp_timebase < pulse_duration).T\n",
    "\n",
    "# - Add padding\n",
    "padding = np.zeros((int(np.round(padding_duration / dt)), 1))\n",
    "chirp = np.vstack((padding, chirp, padding))\n",
    "trigger = np.vstack((padding, trigger, padding))\n",
    "\n",
    "# - Build a time base\n",
    "num_samples = (chirp_end + len(padding)*2) * num_repeats\n",
    "timebase = np.linspace(0, num_samples, num_samples + 1)\n",
    "timebase = timebase[:-1] * dt\n",
    "\n",
    "# - Replicate out inputs and target signals\n",
    "input_t = np.tile(trigger * 1., (num_repeats, 1))\n",
    "target_t = np.tile(chirp, (num_repeats, 1))\n",
    "\n",
    "# - Generate time series objects\n",
    "ts_input = TSContinuous.from_clocked(input_t, dt = dt, periodic=True, name = \"Input\")\n",
    "ts_target = TSContinuous.from_clocked(target_t, dt = dt, periodic=True, name = \"Target\")\n",
    "\n",
    "# - Plot the input and target signals\n",
    "plt.figure()\n",
    "ts_input.plot()\n",
    "ts_target.plot()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Input / target amplitude')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will define a ReLU recurrent module, with a single input and a single output channel, and with a chosen number of units in the recurrent layer ``nResSize``. Larger reservoirs take longer to train, but perform the task to higher accuracy. We'll use the Rockpool modules :py:class:`~.nn.modules.RateJax` and :py:class:`~.nn.modules.LinearJax`, and the combinator :py:class:`~.nn.combinators.Sequential`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of a unit $j$ in the recurrent layer is given by\n",
    "\n",
    "$$\n",
    "\\tau_j\\frac{\\textrm{d}{x_j}}{\\textrm{d}t} + {x_j} = W_r \\cdot f(\\textbf{x}) + b_j + i_j + \\sigma\\zeta_j(t)\n",
    "$$\n",
    "\n",
    "where $\\tau_j$ is the time constant of the unit (`tau`); $W_r$ is the $N \\times N$ weight matrix defining the recurrent connections; $\\textbf{x}$ is the vector of recurrent layer activities (`w_rec`); $f(x)$ is the neuron transfer function $\\tanh(x)$; $b_j$ is the bias input of the unit (`bias`); $i_j$ is the external input to the unit; and $\\sigma\\zeta_j(t)$ is a white noise Wiener process with standard deviation $\\sigma$ (`noise_std`) after 1s.\n",
    "\n",
    "External input is weighted such that $\\textbf{i} = W_i \\cdot i(t)$, where $W_i$ is the external input weight matrix (`w_in`) and $i(t)$ is the external input function.\n",
    "\n",
    "The output of the reservoir is also weighted such that $z = W_o \\cdot \\textbf{x}$, where $W_o$ is the output weight matrix (`w_out`). The goal of the training task is to match the reservoir output $\\hat{z}$ with a target signal $z^*$.\n",
    "\n",
    "Weight initialisation doesn't seem to matter too much in this process; gradient descent does a good job even when weights are initially zero. Here we use a standard initialisation with unit spectral radius for the recurrent weights.\n",
    "\n",
    "You can change the activation function to one of `'tanh'` or `'relu'`. You can also define your own, but must use `jax.numpy` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define the reservoir parameters\n",
    "nResSize = 50\n",
    "activation_func = 'tanh'\n",
    "noise_std = 0.1\n",
    "\n",
    "# - Build a network\n",
    "nInSize = 1\n",
    "nOutSize = 1\n",
    "\n",
    "modRNN = Sequential(\n",
    "    LinearJax((nInSize, nResSize)),\n",
    "    RateJax((nResSize, nResSize),\n",
    "             has_rec = True,\n",
    "             activation_func = activation_func,\n",
    "             dt = dt),\n",
    "    LinearJax((nResSize, nOutSize)),\n",
    ").timed(dt = dt)\n",
    "\n",
    "# - Record initial weights\n",
    "w_rec0 = modRNN._module[1].w_rec\n",
    "tau0 = modRNN._module[1].tau\n",
    "\n",
    "# - Get initial output and plot\n",
    "ts_output0, _, _ = modRNN(ts_input)\n",
    "ts_output0.plot();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Here we use a stochastic gradient-descent optimiser \"Adam\" provided by Jax to optimise all network parameters.\n",
    "\n",
    "First we need to define a loss function. This will be based around the mean-squared error (MSE)between the network output and the target signal, but with an additional harsh penalty for time constants :math:`\\\\tau` that violate a minimum bound. To impose the bounds we use the Rockpool convenience functions :py:func:`~.training.jax_loss.bounds_cost` and :py:func:`~.training.jax_loss.make_bounds`; to compute MSE we use the loss component :py:func:`~.training.jax_loss.mse`; both are available from the Rockpool package via :py:mod:`~.training.jax_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import the convenience functions\n",
    "from rockpool.training.jax_loss import bounds_cost, make_bounds\n",
    "\n",
    "# - Generate a set of pre-configured bounds\n",
    "lower_bounds, upper_bounds = make_bounds(modRNN.module.parameters())\n",
    "print('lower_bounds: ', lower_bounds,\n",
    "      'upper_bounds: ', upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Impose a lower bound for the time constants\n",
    "lower_bounds['1_RateJax']['tau'] = 11 * dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define a constrained MSE loss function\n",
    "import rockpool.training.jax_loss as l\n",
    "\n",
    "def loss_mse(parameters, net, inputs, target):\n",
    "    # - Handle the network state — randomise or reset\n",
    "    net = net.reset_state()\n",
    "    \n",
    "    # - Assign the provided parameters to the network\n",
    "    net = net.set_attributes(parameters)\n",
    "       \n",
    "    # - Evolve the network to get the ouput\n",
    "    output, _, _ = net(inputs)\n",
    "    \n",
    "    # - Impose the bounds\n",
    "    bounds = bounds_cost(parameters, lower_bounds, upper_bounds)\n",
    "    \n",
    "    # - Regularise recurrent weights\n",
    "    reg_l2 = l.l2sqr_norm(parameters['1_RateJax']['w_rec'])\n",
    "    \n",
    "    # - Compute a loss value w.r.t. the target output\n",
    "    return l.mse(output[0], target) + 10. * bounds + reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Useful imports\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "\n",
    "# -- Import an optimiser to use and initalise it\n",
    "import jax\n",
    "from jax.example_libraries.optimizers import adam\n",
    "\n",
    "# - Get the optimiser functions\n",
    "init_fun, update_fun, get_params = adam(1e-4)\n",
    "\n",
    "# - Initialise the optimiser with the initial parameters\n",
    "params0 = deepcopy(modRNN.module.parameters())\n",
    "opt_state = init_fun(params0)\n",
    "\n",
    "# - Get a compiled value-and-gradient function\n",
    "loss_vgf = jax.jit(jax.value_and_grad(loss_mse))\n",
    "\n",
    "# - Compile the optimiser update function\n",
    "update_fun = jax.jit(update_fun)\n",
    "\n",
    "# - Record the loss values over training iterations\n",
    "loss_t = []\n",
    "grad_t = []\n",
    "\n",
    "num_epochs = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Loop over iterations\n",
    "i_trial = count()\n",
    "for _ in tqdm(range(num_epochs)):\n",
    "    # - Get parameters for this iteration\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    # - Get the loss value and gradients for this iteration\n",
    "    loss_val, grads = loss_vgf(params, modRNN.module, input_t, target_t)\n",
    "\n",
    "    # - Update the optimiser\n",
    "    opt_state = update_fun(next(i_trial), grads, opt_state)\n",
    "\n",
    "    # - Keep track of the loss\n",
    "    loss_t.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the loss curve over training\n",
    "plt.plot(loss_t)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training progress');"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Ok, the loss has decreased to a low value. Let's see what the network has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the output of the trained reservoir\n",
    "# - Simulate with trained parameters\n",
    "modRNN._module = modRNN._module.set_attributes(get_params(opt_state))\n",
    "modRNN.reset_all()\n",
    "ts_output, _, record_dict = modRNN(ts_input)\n",
    "\n",
    "# - Compare the output to the target\n",
    "ts_output.plot()\n",
    "ts_target.plot(ts_output.times, ls='--', lw=2)\n",
    "plt.legend();\n",
    "plt.title('Output vs target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone according to plan, the output of the reservoir should closely match the target signal. We can see the effect of training by examining the distribution of network parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSContinuous.from_clocked(record_dict['1_RateJax']['x'][0], dt = dt, name = 'Neuron state').plot(skip =20, stagger = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the network time constants\n",
    "plt.figure()\n",
    "plt.hist(modRNN._module[1].tau / 1e-3, 21);\n",
    "plt.legend(['Trained time constants'])\n",
    "plt.xlabel('Time constant (ms)');\n",
    "plt.ylabel('Count');\n",
    "plt.title('Distribution of time constants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the recurrent layer biases\n",
    "plt.figure()\n",
    "plt.stem(modRNN._module[1].bias);\n",
    "plt.title('Distribution of trained biases')\n",
    "plt.xlabel('Unit')\n",
    "plt.ylabel('Bias');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine something of the computational properties of the network by finding the eigenspectrum of the Jacobian of the recurrent layer. The Jacobian $\\hat{J}$ is given by\n",
    "\n",
    "$$\n",
    "\\hat{J} = (\\hat{W_r} - I) ./ \\hat{T}\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix, $./$ denotes element-wise division, and $T$ is the matrix composed of all time constants $\\hat{\\tau}$ of the recurrent layer.\n",
    "\n",
    "Below we plot the eigenvalues $\\lambda$ of $J$. In my training result, several complex eigenvalues $\\lambda$ with real parts greater than zero are present in the trained eigenspectrum. These correspond to oscillatory modes, which are obviously useful in generating the chirp output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# - Plot the recurrent layer eigenspectrum \n",
    "J = modRNN._module[1].w_rec - np.identity(nResSize)\n",
    "J = J / modRNN._module[1].tau\n",
    "\n",
    "J0 = w_rec0 - np.identity(nResSize)\n",
    "J0 = J0 / tau0\n",
    "\n",
    "plt.figure()\n",
    "eigs = np.linalg.eigvals(J)\n",
    "eigs0 = np.linalg.eigvals(J0)\n",
    "plt.plot(np.real(eigs0), np.imag(eigs0), '.')\n",
    "plt.plot(np.real(eigs), np.imag(eigs), 'o')\n",
    "plt.plot([0, 0], [-100, 100], '--')\n",
    "plt.legend(('Initial eigenspectrum', 'Trained eigenspectrum',))\n",
    "plt.xlim([-350, 250])\n",
    "plt.ylim([-100, 100])\n",
    "plt.title('Eigenspectrum of recurrent weights')\n",
    "plt.xlabel('Re($\\lambda$)')\n",
    "plt.ylabel('Im($\\lambda$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Gradient descent training does a good job of optimmising a dynamic recurrent network for a difficult task requiring significant temporal memory. `jax` provides a computationally efficient back-end as well as automatic differentiation of the recurrent layer. The combination in Rockpool allows us to optimise not just the weights and biases of a network, but also to adapt the neuron dynamics to a desired task."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdbb470bc8a1dcdbddb7a79d90615abccf41de619f1926165544372083c7fcae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
