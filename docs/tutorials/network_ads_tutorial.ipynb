{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building and using an ADS Network\n",
    "Author: Julian Buechel\n",
    "\n",
    "Date: July 1st, 2020\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, we'll be looking at the ADS Network. ADS stands for <b>A</b>rbitrary <b>D</b>ynamical <b>S</b>ystems and is referring to the fact that this network is able to learn any non-linear dynamical system of the form\n",
    "\n",
    "$$\\dot{x} = f(x) + c$$\n",
    "\n",
    "where $c$ is a continuous input and $f(.)$ some arbitrary non-linear function.\n",
    "\n",
    "Being able to learn how to implement an arbitrary dynamical system in a spiking network is a powerful ability, which can be exploited to do more complex tasks, such as training a classifier for temporal tasks like wake-phrase detection or keyword spotting.\n",
    "\n",
    "## Background\n",
    "<b>E</b>fficient <b>B</b>alanced <b>N</b>etworks (<b>EBN</b>) [1](https://www.nature.com/articles/nn.4243) have coding properties that are of high interest to the neuromorphic community: They encode signals in an extremely precise and sparse way while still being robust to various types of noise.\n",
    "\n",
    "This architecture is actually really easy as it doesn't require any training. In 2017, Alemi et al. [2](https://arxiv.org/abs/1705.08026) have demonstrated that a spiking network can be trained to follow the non-linear dynamics of a teacher system while still keeping tight balance and robustness to sudden neuron death.\n",
    "\n",
    "We adapted their learning rule from adaptive non-linear control theory and stripped down their architecture to the bare minimum of complexity (we for example got rid of the dendritic non-linearities) in order to use it to our advantage.\n",
    "\n",
    "## Technical details\n",
    "With the goal in mind to train a classifier using this network architecture, we realised that a classic rate network with the following dynamics\n",
    "$$\\tau_j \\dot{x}_j = -x_j + \\hat{\\mathbf{F}}c(t)_j + \\hat{\\mathbf{\\Omega}}f(x(t))_j + b_j + \\epsilon_j$$\n",
    "actually implements a function of the form\n",
    "$$\\dot{x} = f(x) + c$$\n",
    "which can be learned by the ADS Network!\n",
    "This means that we can train a rate network using BPTT (backpropagation through time) and then train a spiking ADS Network to mimic the rate networks dynamics.\n",
    "\n",
    "![Caption](images/learning_setup.png)\n",
    "\n",
    "This image portraits the setup during learning for an audio task: The raw audio sample is filtered using a filterbank and then fed through the linear feed-forward matrix of the trained rate networks, here denoted $\\hat{F}$. This signal is then fed into the rate network, where the rate network produces some dynamics $x$ in response to the input. Since the rate network is already trained, these dynamics exhibit a useful pattern that can be decoded using the, also trained, read-out weights $\\hat{D}$, which are shown in the figure below.\n",
    "Now comes the spiking ADS network with $N$ neurons, which is typically 6 times the number of rate units. The transformed signal $\\hat{F}c$ is fed into the spiking network through another encoder $F$. The plastic weights $\\mathbf{\\Omega^s}$ are then trained so the the output $\\hat{x} = Dr$ matches the dynamics of the rate network.\n",
    "The great thing is that $D$ is fixed and pre-defined: It is simply the transpose of $F$.\n",
    "\n",
    "![Caption](images/inference_setup.png)\n",
    "\n",
    "During inference, the rate network does not matter anymore and we can feed the audio signal through the network using the feed-forward matrix $F\\hat{F}$ and the decoder matrix $\\hat{D}D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal XOR\n",
    "We will now show how to train a rate network on a 1D temporal XOR task and then train an ADS network to implement the dynamics of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 0.5\n",
    "matplotlib.rcParams['lines.markersize'] = 0.5\n",
    "matplotlib.rcParams['figure.figsize'] = 15, 5\n",
    "import matplotlib.pyplot as plt\n",
    "from rockpool.layers import RecRateEulerJax_IO, H_tanh\n",
    "from rockpool.networks import NetworkADS\n",
    "from rockpool.timeseries import TSContinuous\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "from typing import Dict, Tuple, Any, Callable, Union, List, Optional\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some helper functions\n",
    "# Moving average for smoothing the response\n",
    "def filter_1d(data, alpha = 0.9):\n",
    "    last = data[0]\n",
    "    out = np.zeros((len(data),))\n",
    "    out[0] = last\n",
    "    for i in range(1,len(data)):\n",
    "        out[i] = alpha*out[i-1] + (1-alpha)*data[i]\n",
    "        last = data[i]\n",
    "    return out\n",
    "\n",
    "def generate_xor_sample(total_duration, dt, amplitude=1, use_smooth=True, plot=False):\n",
    "    \"\"\"\n",
    "    Generates a temporal XOR signal\n",
    "    \"\"\"\n",
    "    input_duration = 2/3*total_duration\n",
    "    # Create a time base\n",
    "    t = np.linspace(0,total_duration, int(total_duration/dt)+1)\n",
    "    first_duration = np.random.uniform(low=input_duration/10, high=input_duration/4 )\n",
    "    second_duration = np.random.uniform(low=input_duration/10, high=input_duration/4 )\n",
    "    end_first = np.random.uniform(low=first_duration, high=2/3*input_duration-second_duration)\n",
    "    start_first = end_first - first_duration\n",
    "    start_second = np.random.uniform(low=end_first + 0.1, high=2/3*input_duration-second_duration) # At least 200 ms break\n",
    "    end_second = start_second+second_duration\n",
    "    data = np.zeros(int(total_duration/dt)+1)\n",
    "    i1 = np.random.rand() > 0.5\n",
    "    i2 = np.random.rand() > 0.5\n",
    "    response = (((not i1) and i2) or (i1 and (not i2)))\n",
    "    if(i1):\n",
    "        a1 = 1\n",
    "    else:\n",
    "        a1 = -1\n",
    "    if(i2):\n",
    "        a2 = 1\n",
    "    else:\n",
    "        a2 = -1\n",
    "    input_label = 0\n",
    "    if(a1==1 and a2==1):\n",
    "        input_label = 0\n",
    "    elif(a1==1 and a2==-1):\n",
    "        input_label = 1\n",
    "    elif(a1==-1 and a2==1):\n",
    "        input_label = 2\n",
    "    else:\n",
    "        input_label = 3\n",
    "    data[(start_first <= t) & (t < end_first)] = a1\n",
    "    data[(start_second <= t) & (t < end_second)] = a2\n",
    "    if(use_smooth):\n",
    "        sigma = 10\n",
    "        w = (1/(sigma*np.sqrt(2*np.pi)))* np.exp(-((np.linspace(1,1000,int(1/dt))-500)**2)/(2*sigma**2))\n",
    "        w = w / np.sum(w)\n",
    "        data = amplitude*np.convolve(data, w, \"same\")\n",
    "    else:\n",
    "        data *= amplitude\n",
    "    target = np.zeros(int(total_duration/dt)+1)\n",
    "    if(response):\n",
    "        ar = 1.0\n",
    "    else:\n",
    "        ar = -1.0\n",
    "    target[int(1/dt*(end_second+0.05)):int(1/dt*(end_second))+int(1/dt*0.3)] = ar\n",
    "    sigma = 20\n",
    "    w = (1/(sigma*np.sqrt(2*np.pi)))* np.exp(-((np.linspace(1,1000,int(1/dt))-500)**2)/(2*sigma**2))\n",
    "    w = w / np.sum(w)\n",
    "    target = np.convolve(target, w, \"same\")\n",
    "    target /= np.max(np.abs(target))\n",
    "    if(plot):\n",
    "        eps = 0.05\n",
    "        plt.subplot(211)\n",
    "        plt.plot(t, data)\n",
    "        plt.ylim([-amplitude-eps, amplitude+eps])\n",
    "        plt.subplot(212)\n",
    "        plt.plot(t, target)\n",
    "        plt.show()\n",
    "    return (data[:int(total_duration/dt)], target[:int(total_duration/dt)], input_label)\n",
    "\n",
    "def k_step_function(total_num_iter, step_size, start_k):\n",
    "    stop_k = step_size\n",
    "    num_reductions = int((start_k - stop_k) / step_size) + 1\n",
    "    reduce_after = int(total_num_iter / num_reductions)\n",
    "    reduction_indices = [i for i in range(1,total_num_iter) if (i % reduce_after) == 0]\n",
    "    k_of_t = np.zeros(total_num_iter)\n",
    "    if(total_num_iter > 0):\n",
    "        k_of_t[0] = start_k\n",
    "        for t in range(1,total_num_iter):\n",
    "            if(t in reduction_indices):\n",
    "                k_of_t[t] = k_of_t[t-1]-step_size\n",
    "            else:\n",
    "                k_of_t[t] = k_of_t[t-1]\n",
    "\n",
    "    return k_of_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration of the input data\n",
    "_,_,_ = generate_xor_sample(total_duration=1.0, dt=0.001, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Initialize some variables\n",
    "# - Define global parameters\n",
    "verbose = 1 # - Level of verbosity also for the ADS Network (use 1 for this tutorial)\n",
    "activation_func = H_tanh # - Should be kept to tanh\n",
    "duration = 1.0 # - Duration of the signal in seconds\n",
    "dt = 1e-3 # - Simulation time step in seconds\n",
    "amplitude = 1.0\n",
    "num_units = 64 # - Corresponds to the number of neurons in the rate network\n",
    "noise_std = 0.0\n",
    "num_epochs = 100\n",
    "num_batches = 100\n",
    "num_channels = 1 # - Number of input channels. Here 1 for the 1D input\n",
    "num_targets = 1 # - Output dimension is also 1D\n",
    "time_base = np.arange(0,duration,dt)\n",
    "\n",
    "# - Create the rate network\n",
    "w_in = 10.0 * (np.random.rand(num_channels, num_units) - .5)\n",
    "w_rec = 0.2 * (np.random.rand(num_units, num_units) - .5)\n",
    "w_rec -= np.eye(num_units) * w_rec\n",
    "\n",
    "w_out = 0.4*np.random.uniform(size=(num_units, num_targets))-0.2\n",
    "bias = 0.0 * (np.random.rand(num_units) - 0.5)\n",
    "tau = np.linspace(0.01, 0.1, num_units)\n",
    "\n",
    "sr = np.max(np.abs(np.linalg.eigvals(w_rec)))\n",
    "w_rec = w_rec / sr * 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"./Resources\")):\n",
    "    print(\"Creating Resources folder\"); os.mkdir(\"./Resources\")\n",
    "\n",
    "model_path = \"./Resources/temporal_xor_rate_model_64.json\"\n",
    "loaded_pretrained = False\n",
    "if(os.path.exists(model_path)):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        loaded_net = json.load(f)\n",
    "        if(loaded_net['activation_func'] == 'tanh'):\n",
    "            activation_func = H_tanh\n",
    "        else:\n",
    "            assert(False), \"Please use tanh\"\n",
    "        lyr_hidden = RecRateEulerJax_IO(activation_func=activation_func,\n",
    "                                        w_in=loaded_net['w_in'],\n",
    "                                        w_recurrent=loaded_net['w_recurrent'],\n",
    "                                        w_out=loaded_net['w_out'],\n",
    "                                        tau=loaded_net['tau'],\n",
    "                                        bias=loaded_net['bias'],\n",
    "                                        dt=loaded_net['dt'],\n",
    "                                        noise_std=loaded_net['noise_std'],\n",
    "                                        name=\"hidden\")\n",
    "        loaded_pretrained = True\n",
    "        w_out = loaded_net['w_out']\n",
    "        print(\"Loaded pretrained layer\")\n",
    "\n",
    "else:\n",
    "    # - Create the rate layer\n",
    "    lyr_hidden = RecRateEulerJax_IO(activation_func=activation_func,\n",
    "                                                w_in=w_in,\n",
    "                                                w_recurrent=w_rec,\n",
    "                                                w_out=w_out,\n",
    "                                                tau=tau,\n",
    "                                                bias=bias,\n",
    "                                                dt=dt,\n",
    "                                                noise_std=noise_std,\n",
    "                                                name=\"hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not loaded_pretrained):\n",
    "    # - Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        num_samples = 0\n",
    "        mvg_avg_mse = 0\n",
    "\n",
    "        for batch_id in range(num_batches):\n",
    "\n",
    "            # - Generate new training data\n",
    "            data, target, _ = generate_xor_sample(total_duration=duration, dt=dt, amplitude=amplitude)\n",
    "            ts_data = TSContinuous(time_base, data)\n",
    "            ts_target = TSContinuous(time_base, target)\n",
    "\n",
    "            lyr_hidden.reset_time()\n",
    "            l_fcn, g_fcn, o_fcn = lyr_hidden.train_output_target(ts_data,\n",
    "                                                                    ts_target,\n",
    "                                                                    is_first = (batch_id == 0) and (epoch == 0),\n",
    "                                                                    opt_params={\"step_size\": 1e-4})\n",
    "\n",
    "            ts_out = lyr_hidden.evolve(ts_data)\n",
    "\n",
    "            if(verbose > 0):\n",
    "                clear_output(wait=True)\n",
    "                plt.clf()\n",
    "                ts_target.plot(linestyle='--')\n",
    "                ts_out.plot()\n",
    "                plt.draw()\n",
    "                plt.pause(0.0001)\n",
    "\n",
    "            mse = np.linalg.norm(ts_target.samples-ts_out.samples)**2\n",
    "            mvg_avg_mse = mvg_avg_mse * num_samples + mse\n",
    "            num_samples += 1\n",
    "            mvg_avg_mse /= num_samples\n",
    "\n",
    "            print(f\"Moving average is {mvg_avg_mse} tau max {np.max(lyr_hidden.tau)} mean {np.mean(lyr_hidden.tau)}\")\n",
    "            print(f\"bias max {np.max(lyr_hidden.bias)} mean {np.mean(lyr_hidden.bias)}\")\n",
    "    lyr_hidden.reset_all()\n",
    "    lyr_hidden.noise_std = 0.0\n",
    "    \n",
    "    # - Save rate net\n",
    "    lyr_hidden.save_layer(model_path)\n",
    "    print(\"Saved layer\")\n",
    "else:\n",
    "    print(\"No training, pretrained layer was loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define parameters used by the ADS Network\n",
    "\n",
    "Nc = num_units\n",
    "num_neurons = 768 # - This can be varied\n",
    "\n",
    "dt = 0.001\n",
    "duration = 1.0\n",
    "time_base = np.arange(0,duration,dt)\n",
    "lambda_d = 20\n",
    "lambda_v = 20\n",
    "tau_mem = 1/ lambda_v\n",
    "\n",
    "tau_slow = 0.07 # 70ms\n",
    "tau_out = 0.07\n",
    "\n",
    "# - See Boerlin, Machens and Deneve for the calculations below\n",
    "tau_syn_fast = tau_slow\n",
    "mu = 0.0005\n",
    "nu = 0.0001\n",
    "D = np.random.randn(Nc,num_neurons) / Nc\n",
    "weights_fast = (D.T@D + mu*lambda_d**2*np.eye(num_neurons))\n",
    "# - Start with zero weights \n",
    "weights_slow = np.zeros((num_neurons,num_neurons))\n",
    "\n",
    "eta = 0.0001\n",
    "k = 100\n",
    "# - Pull out thresholds\n",
    "v_thresh = (nu * lambda_d + mu * lambda_d**2 + np.sum(abs(D.T), -1, keepdims = True)**2) / 2\n",
    "# - Fill the diagonal with zeros\n",
    "np.fill_diagonal(weights_fast, 0)\n",
    "\n",
    "# - Calculate weight matrices for realistic neuron settings\n",
    "v_thresh_target = 1.0*np.ones((num_neurons,)) # - V_thresh\n",
    "v_rest_target = 0.5*np.ones((num_neurons,)) # - V_rest = b\n",
    "\n",
    "b = v_rest_target\n",
    "a = v_thresh_target - b\n",
    "\n",
    "# - Feedforward weights: Divide each column i by the i-th threshold value and multiply by i-th value of a\n",
    "D_realistic = a*np.divide(D, v_thresh.ravel())\n",
    "weights_in_realistic = D_realistic\n",
    "weights_out_realistic = np.copy(D_realistic).T\n",
    "weights_fast_realistic = a*np.divide(weights_fast.T, v_thresh.ravel()).T # - Divide each row\n",
    "\n",
    "# - Reset is given by v_reset_target = b - a\n",
    "v_reset_target = b - a\n",
    "noise_std_realistic = 0.00\n",
    "\n",
    "# - Initialize network\n",
    "# - Use discretize_dynapse=True if the DYNAP-SE II constraints should be used\n",
    "net = NetworkADS.SpecifyNetwork(N=num_neurons,\n",
    "                                Nc=Nc,\n",
    "                                Nb=num_neurons,\n",
    "                                weights_in= weights_in_realistic * tau_mem,\n",
    "                                weights_out= weights_out_realistic,\n",
    "                                weights_fast= - weights_fast_realistic / tau_syn_fast * 0, # - We will not use the fast recurrent connections here\n",
    "                                weights_slow = weights_slow,\n",
    "                                eta=eta,\n",
    "                                k=k,\n",
    "                                noise_std=noise_std_realistic,\n",
    "                                dt=dt,\n",
    "                                v_thresh=v_thresh_target,\n",
    "                                v_reset=v_reset_target,\n",
    "                                v_rest=v_rest_target,\n",
    "                                tau_mem=tau_mem,\n",
    "                                tau_syn_r_fast=tau_syn_fast,\n",
    "                                tau_syn_r_slow=tau_slow,\n",
    "                                tau_syn_r_out=tau_out,\n",
    "                                record=True)\n",
    "\n",
    "amplitude = 10 / tau_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function\n",
    "This helper function passes the temporal-XOR input through the rate network and returns:\n",
    "\n",
    "- The transformed input $\\hat{F}c$ that gets fed into the spiking network\n",
    "- The rate network dynamics $x$ that the spiking network needs to generate\n",
    "- And the output of the rate network for plotting and error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "        ts_data = TSContinuous(time_base, data)\n",
    "        # - Pass through the rate network\n",
    "        ts_rate_out = lyr_hidden.evolve(ts_data)\n",
    "        lyr_hidden.reset_all()\n",
    "        # - Get the target dynamics\n",
    "        ts_rate_net_target_dynamics = lyr_hidden.res_acts_last_evolution\n",
    "        # - Get the input into the spiking network\n",
    "        ts_spiking_in = TSContinuous(lyr_hidden.res_inputs_last_evolution.times,amplitude*lyr_hidden.res_inputs_last_evolution.samples)\n",
    "        return (ts_spiking_in, ts_rate_net_target_dynamics, ts_rate_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Test the function\n",
    "\n",
    "ts_test_spiking_in, ts_test_rate_dynamics, ts_test_rate_out = get_data(data=generate_xor_sample(total_duration=duration, dt=dt)[0])\n",
    "plt.figure(figsize=(15,10)); ts_test_rate_dynamics.plot(stagger=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create schedules for eta and k\n",
    "\n",
    "num_epochs = 5\n",
    "num_samples_per_epoch = 1000 # TODO Change that to 500-1000\n",
    "\n",
    "# - Create step schedule for k\n",
    "step_size = net.lyrRes.k_initial / 8\n",
    "total_num_iter = num_epochs*num_samples_per_epoch\n",
    "k_of_t = k_step_function(total_num_iter=total_num_iter,\n",
    "                            step_size=step_size,\n",
    "                            start_k = net.lyrRes.k_initial)\n",
    "if(total_num_iter > 0):\n",
    "    f_k = lambda t : np.maximum(step_size,k_of_t[t])\n",
    "    if(verbose > 0):\n",
    "        plt.plot(np.arange(0,total_num_iter),f_k(np.arange(0,total_num_iter))); plt.title(\"Decay schedule for k\"); plt.show()\n",
    "else:\n",
    "    f_k = lambda t : 0\n",
    "    \n",
    "# - Create schedule for eta\n",
    "a_eta = net.lyrRes.eta_initial\n",
    "b_eta = (total_num_iter/2) / np.log(100)\n",
    "c_eta = 0.0000001\n",
    "f_eta = lambda t,a_eta,b_eta : a_eta*np.exp(-t/b_eta) + c_eta\n",
    "\n",
    "if(verbose > 0):\n",
    "    plt.plot(np.arange(0,total_num_iter),f_eta(np.arange(0,total_num_iter),a_eta,b_eta))\n",
    "    plt.title(\"Decay schedule for eta\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about training\n",
    "There are a few things to keep in mind when training an ADS Network:\n",
    "\n",
    "- During training, an error-dependent current $kD^Te$ is fed into the neurons. This current steers they dynamics into the correct direction so that they follow the target dynamics. This will lead to a small reconstruction error during training, but does not mean that the reconstruction error is low during validation (when the current is suddently gone).\n",
    "- The hyperparameter $k$ should be chosen in a way that the magnitudes of error- and output current match. As stated before, a too big error current will lead to no generalization\n",
    "- Scale of the output weights: This is a hyperparameter that turned out important. For smooth target dynamics a scale < 1 (e.g. 0.5) is good and for more sharp and abrupt dynamics a scalar >= 1 is good (like in this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Do the training of the ADS Network\n",
    "%matplotlib qt\n",
    "\n",
    "np.random.seed(42)\n",
    "time_horizon = 50\n",
    "recon_errors = np.ones((time_horizon,))\n",
    "avg_training_acc = np.zeros((time_horizon,)); avg_training_acc[:int(time_horizon/2)] = 1.0\n",
    "was_first = False\n",
    "\n",
    "\n",
    "def validate():\n",
    "    # - Evolve over new examples without learning\n",
    "    for _ in range(10):\n",
    "        data, target, _ = generate_xor_sample(total_duration=duration, dt=dt, amplitude=1.0)\n",
    "        (ts_spiking_in, _, ts_rate_out) = get_data(data=data)\n",
    "        test_sim = net.evolve(ts_input = ts_spiking_in, verbose = False)\n",
    "        out_val = test_sim[\"output_layer_0\"].samples\n",
    "        final_out = out_val @ lyr_hidden.w_out\n",
    "        final_out = filter_1d(final_out, alpha=0.95)\n",
    "        clear_output(wait=True)\n",
    "        plt.clf()\n",
    "        plt.plot(time_base, final_out, label=\"Spiking\")\n",
    "        plt.plot(time_base, target, label=\"Target\")\n",
    "        plt.plot(time_base, ts_rate_out.samples, label=\"Rate\")\n",
    "        plt.axhline(y=0.5, color=\"r\")\n",
    "        plt.axhline(y=-0.5, color=\"r\")\n",
    "        plt.ylim([-1.5,1.5])\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "        plt.pause(0.5)\n",
    "        net.reset_all()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    validate()\n",
    "    \n",
    "    for batch_id in range(num_samples_per_epoch):\n",
    "        \n",
    "        # - Get data\n",
    "        data, target, _ = generate_xor_sample(total_duration=duration, dt=dt, amplitude=1.0)\n",
    "        (ts_spiking_in, ts_rate_net_target_dynamics, ts_rate_out) = get_data(data=data)\n",
    "        \n",
    "        train_sim = net.train_step(ts_input = ts_spiking_in,\n",
    "                                   ts_target = ts_rate_net_target_dynamics,\n",
    "                                   k = f_k(epoch*num_samples_per_epoch+batch_id),\n",
    "                                   eta = f_eta(epoch*num_samples_per_epoch+batch_id, a_eta, b_eta),\n",
    "                                   verbose = False)\n",
    "\n",
    "        # - Compute train loss & update the moving averages\n",
    "        out_val = train_sim[\"output_layer_0\"].samples\n",
    "        target_val = ts_rate_net_target_dynamics.samples\n",
    "        error = np.sum(np.var(target_val-out_val, axis=0, ddof=1)) / (np.sum(np.var(target_val, axis=0, ddof=1)))\n",
    "        recon_errors[1:] = recon_errors[:-1]\n",
    "        recon_errors[0] = error\n",
    "        # - Initialize the error array with the first reconstruction error we get\n",
    "        if(not was_first):\n",
    "            was_first = True\n",
    "            recon_errors = [error for _ in range(time_horizon)]\n",
    "\n",
    "        # - Compute the final output using the reconstructed dynamics and the trained read-out of the rate net\n",
    "        final_out = out_val @ lyr_hidden.w_out\n",
    "        # - ..and filter\n",
    "        final_out = filter_1d(final_out, alpha=0.95)\n",
    "\n",
    "        # - Some plotting\n",
    "        if(verbose > 0):\n",
    "            clear_output(wait=True)\n",
    "            plt.clf()\n",
    "            plt.plot(time_base, final_out, label=\"Spiking\")\n",
    "            plt.plot(time_base, target, label=\"Target\")\n",
    "            plt.plot(time_base, ts_rate_out.samples, label=\"Rate\")\n",
    "            plt.ylim([-1.5,1.5])\n",
    "            plt.legend()\n",
    "            plt.draw()\n",
    "            plt.pause(0.001)\n",
    "        \n",
    "        print((\"Avg. reconstruction error %.4f\" % (np.mean(recon_errors))), \"K\", net.lyrRes.k, \"Epoch\", epoch, \"Batch\", batch_id)\n",
    "net.reset_all()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('ai-ctx': conda)",
   "language": "python",
   "name": "python36764bitaictxconda29610f70ff0d40aab7c98a62ec108980"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
