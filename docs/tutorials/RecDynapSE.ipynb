{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-based simulation on `DynapSE` hardware"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This document illustrates how to use the `.RecDynapSE` layer, that runs directly on the DynapSE event-driven neuromorphic computing hardware from aiCTX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware basics\n",
    "The layer uses the DynapSE processor, which consists of 4 chips. Each chips has 4 cores of 256 neurons. The chips, as well as each core in a chip and each neuron in a core are identified with an ID between 0 and 3 or 0 and 256, respectively. However, for this layer the neurons are given logical IDs from 0 to 4095 that range over all neurons. In other words the logical neuron ID is $1024 \\cdot \\text{ChipID} + 256 \\cdot \\text{CoreID} + \\text{NeuronID}$.\n",
    "\n",
    "\n",
    "## Setup\n",
    "### Connecting to Cortexcontrol\n",
    "In order to work interface the DynapSE chip, this layer relies on `cortexcontrol`. It should be accessed via an `RPyC` connection. In order to run some examples from within this jupyter notebook, we will do the latter. For this we start `cortexcontrol` and run the following commands in its console (not in this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpyc.utils.classic\n",
    "c = rpyc.utils.classic.SlaveService()\n",
    "from rpyc.utils.server import OneShotServer\n",
    "t = OneShotServer(c, port=1300)\n",
    "print(\"RPyC: Ready to start.\")\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `cortexcontrol` console prints <br>\n",
    "<i>\"RPyC: Ready to start.\"</i> <br>\n",
    "and nothing else, it is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DynapseControl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The `.RecDynapSE` layer uses a `.DynapseControl` object to interact with `Cortexcontrol` (see .... for more details). You can either pass an existing `DynapseControl` object to the layer upon instantiation, or let the layer create such an object. Either way, it can be accessed via the layer's `controller` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Import the class with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# - Import recurrent RecDIAF layer\n",
    "from rockpool.layers import RecDynapSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a while until the `dynapse_control` module has prepared the hardware.\n",
    "\n",
    "## Instantiation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "`RecDynapSE` objects have many instantiation arguments in common with other |project| layers, in partuclar recurrent layers such as `.RecIAFTorch`, `.RecIAFBrian` or `.RecDIAF`. Other arguments are related to how the hardware is used and therefore unique to this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument | Type | Default | Meaning |\n",
    "|----------|------|---------|---------|\n",
    "| `weights_in` | 2D-`ndarray`| - | Input weights (required) |\n",
    "| `weights_rec` | 2D-`ndarray` | - | Recurrent weights (required) |\n",
    "| `neuron_ids` | 1D-`ArrayLike` | `None` | IDs of the layer neurons |\n",
    "| `virtual_neuron_ids` | 1D-`ArrayLike` | `None` | IDs of the virtual (input) neurons |\n",
    "| `dt` | `float` | `2e-5` | Time step |\n",
    "| `max_num_trials_batch` | `int` | `None` | Maximum number of trials in individual batch |\n",
    "| `max_batch_dur` | `float` | `None` | Maximum duration time of individual batch |\n",
    "| `max_num_timesteps` | `int` | `None` | Maximum number of time steps in individual batch |\n",
    "| `max_num_events_batch` | `int` | `None` | Maximum number of input Events during individual batch |\n",
    "| `l_input_core_ids` | `ArrayLike` | `[0]` | IDs of cores that receive input spikes |\n",
    "| `input_chip_id` | `int` | 0 | Chip that receives input spikes |\n",
    "| `clearcores_list` | `ArrayLike` | `None` | IDs of cores to be reset |\n",
    "| `controller` | `DynapseControl` | `None` | `DynapseControl` instance |\n",
    "| `rpyc_port` | `int` or `None` | `None` | Port for RPyC connection |\n",
    "| `name` | `str` | \"unnamed\" | Layer name |\n",
    "\n",
    "`weights_in` and `weights_rec` are the input and recurrent weights and have to be provided as 2D-arrays. `weights_in` determines the layer's dimensions `nSIzeIn` and `size`. `weights_rec` has to be of size `size` x `size`. Each weight must be an integer (positive or negative). Furthermore, the sum over the absolute values of the elements in any given column in `weights_in` plus the sum over the absolute values of elements in the corresponding column of `weights_rec` must be les or equal to 64. $\\sum_{i} |$ `weights_in` $_{ik}| + \\sum_{j}|$ `weights_rec` $_{jk}| \\leq 64~\\forall k$. This is due to limitations of the hardware and cannot be circumvented."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The layer neurons of `.RecDynapSE` objects directly correspond to physical neurons on the chip. Inputs are sent to the hardware through so-called virtual neurons. Each virtual neuron has an ID, just like a physical neuron. Each input channel of the layer corresponds to such a virtual neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose which physical and virtual neurons are used for the layer by passing their IDs in `vnLayerNeuronIDs` and `vnVirtualNeuronIDs`, which are 1D array-like objects with integers between 0 and 1023 or 0 and 4095, respectively. All neurons with IDs from $j \\cdot 256$ to $(j+1) \\cdot 256, \\ j \\in {0,1,..15}$ belong to the same core (with core ID $j \\ mod \\ 4$). All neurons with IDs from $j \\cdot 1024$ to $(j+1) \\cdot 1024, \\ j \\in {0,..,4}$ belong to same chip (with chip ID $j$). With this in mind you can allocate neurons to specific chips and cores. \n",
    "\n",
    "<!---The n-th entry of `vnLayerNeuronIDs` corresponds to n-th layer neuron, the n-th entry of `vnVirtualNeurons` corresponds to the n-th input channel of the layer. As described below, under <i>Neurons</i>, it is recommended that the IDs of the physical neurons be all different to the IDs of the virtual neurons. If these arguments are `None`, the neurons will be automatically allocated. -->\n",
    "\n",
    "In order for the to layer to function as expected you should stick to the following two rules:\n",
    "- **Neurons that receive external input should be on different cores than  neurons that receive input from other layer neurons. As a consequence, each neuron should not receive both types of input.** The cores with neurons that receive external inputs are set with `l_input_core_ids`.\n",
    "- **All neurons that receive external input should be on the same chip.** This chip is set with `input_chip_id` and is 0 by default.\n",
    "\n",
    "\n",
    "*These rules are quite restrictive and it is possible to set them less strictly. Contact Felix if needed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dt` is a positive `float` that on the one hand sets the discrete layer evolution timestep, as in other layers, but on the other hand also corresponds to the smallest (nonzero) interval between input events that are sent to the chip. It needs to be larger than $1.11 \\cdot 10^{-9}$ (seconds). Below, under *Choosing dt* you can find some more thoughts on how to choose this value.\n",
    "\n",
    "Evolution is automatically split into batches, the size of which is determined by `max_num_events_batch`, `max_num_timesteps`, `max_batch_dur` and `max_num_trials_batch`, which control the maximum number of events, number of timesteps, duration or number of trials in a batch.  All of them can be set to `None`, which corresponds to setting no limit, except for `max_num_events_batch`, where the limit will be set to 65535. If both `max_num_timesteps` and `max_batch_dur` are not `None`, the `max_batch_dur` will be ignored. `max_num_trials_batch` will only have an effect when the input time series to the `evolve` method contains a `trial_start_times` attribute. For more details on how evolutions are split into batches see <i>Evolution in batches</i>.\n",
    "\n",
    "The list `clearcores_list` contains the IDs of the cores where (presynaptic) connections should be reset on instantiation. Ideally this contains all the cores that are going to be used for this `RecDynapSE` object. However, if you want to save time and you know what you are doing you can set this to `None`, so no connections are reset.\n",
    "\n",
    "You can pass an existing `DynapseControl` instance to the layer that will handle the interactions with `Cortexcontrol`. If this argument is `None`, a new `DynapseControl` will be instantiated. In this case, you can use the `rpyc_port` argument to define a port at which it should try to establish the connection."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "`.RecDynapSE.name` is a `str` that defines the layer's name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these values can be accessed and changed via `<Layer>.value`, where `<Layer>` is the instance of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "We can set up a simple layer on the chip by only passing input weights and recurrent weights. The weights are chosen so that there is a population of 3 \"input neurons\" that receive the input and then excite the remaining 6 neurons, which are recurrently connected. This way the constraints mentioned above are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecDynapSE `example-layer`: Superclass initialized\n",
      "dynapse_control: RPyC connection established through port 1300.\n",
      "dynapse_control: RPyC namespace complete.\n",
      "dynapse_control: RPyC connection has been setup successfully.\n",
      "DynapseControl: Initializing DynapSE\n",
      "DynapseControl: Spike generator module ready.\n",
      "DynapseControl: Poisson generator module ready.\n",
      "DynapseControl: Time constants of cores [] have been reset.\n",
      "DynapseControl: Neurons initialized.\n",
      "\t 0 hardware neurons and 1023 virtual neurons available.\n",
      "DynapseControl: Neuron connector initialized\n",
      "DynapseControl: Connectivity array initialized\n",
      "DynapseControl: FPGA spike generator prepared.\n",
      "DynapseControl ready.\n",
      "DynapseControl: Not sufficient neurons available. Initializing  chips to make more neurons available.\n",
      "dynapse_control: Chips 0 have been cleared.\n",
      "DynapseControl: 1023 hardware neurons available.\n",
      "Layer `example-layer`: Layer neurons allocated\n",
      "Layer `example-layer`: Virtual neurons allocated\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between virtual and hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between virtual and hardware neurons have been set.\n",
      "Layer `example-layer`: Connections to virtual neurons have been set.\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "Layer `example-layer`: Connections from input neurons to reservoir have been set.\n",
      "DynapseControl: Excitatory connections of type `SLOW_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "DynapseControl: Connections have been written to the chip.\n",
      "Layer `example-layer`: Recurrent connections have been set.\n",
      "Layer `example-layer` prepared.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# - Weight matrices: 3 neurons receive external input \n",
    "#   from 2 channels and stimulate the remaning\n",
    "#   6 neurons, which are recurrently connected.\n",
    "\n",
    "weights_in = np.zeros((2,9))\n",
    "# Only first 3 neurons receive input\n",
    "weights_in[:,:3] = np.random.randint(-2, 2, size=(2,3))\n",
    "\n",
    "weights_rec = np.zeros((9,9))\n",
    "# Excitatory connections from input neurons to rest\n",
    "weights_rec[:3, 3:] = np.random.randint(3, size=(3,6))\n",
    "# Recurrent connecitons between remaining 6 neurons\n",
    "weights_rec[3:, 3:] = np.random.randint(-2, 2, size=(6,6))\n",
    "\n",
    "rlRec = RecDynapSE(weights_in, weights_rec, l_input_core_ids=[0], name=\"example-layer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing dt\n",
    "As with all layers, a `RecDynapSE` object's evolution takes place in discrete time steps. This allows to send an `num_timesteps` argument to the `evolve` method, which is consistent with other layer classes and important for the use within a `Network`. Besides, event though the hardware evolves in continuous time, the input events use discrete timesteps. These timesteps are $\\frac{10^{-7}}{9} \\text{ s} = 11.\\bar{1} \\cdot 10^{-9} \\text{ s}$ and mark the smallest value that can be chosen for the layer timestep `dt`.\n",
    "\n",
    "Although the effect the timestep size has on computation time is much smaller than with other layers, it is not always advisable to choose the smallest possible value. The reason is that the number of timesteps between two input spikes is currently limited to $2^{16}-1 = 65535$. This means that with dt $= 11.\\overline{1} \\cdot 10^{-9} \\text{ s}$, any section in the  input signal without input spikes longer than about 0.73 milliseconds will cause the layer to throw an exception. Therefore it makes sense to set `dt` to something between $10^{-6}$ and $10^{-4}$ seconds, in order to allow for sufficiently long silent parts in the input while still maintaining a good temporal resolution.\n",
    "\n",
    "**If these limitations are causing problems contact Felix so that he can implement a way around it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons\n",
    "The layer neurons of `RecDynapSE` objects directly correspond to physical neurons on the chip. Inputs are sent to the hardware through so-called virtual neurons. Each input channel of the layer corresponds to such a virtual neuron. Every neuron on the DynapSE has a logical ID, which ranges from 0 to 4095 for the physical and from 0 to 1023 for the virtual neurons. \n",
    "\n",
    "<!--- (probably not necessary) An input spike to the layer translates to a virtual neuron emitting a spike. If a neuron is set to receive spikes from a neuron with ID $n$, it makes no difference if a virtual or a pysical neuron with this ID is firing. The target neuron will receive the spikes in both cases. Because this can cause unexpected behavior it is recommended that the IDs of the physical neurons are pairwise different from the IDs of of the virtual neurons.\n",
    "--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron states\n",
    "Hardware neurons' states change constantly according to the laws of physics, even when the layer is currently not evolving, and there is no state variable that could be read out for all neurons simultaneously. Therefore the `RecDynapSE` has no state vector like other layer classes. The `state` attribute is just a 1D array of `size` zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synapses\n",
    "\n",
    "There are four different synapse types on the DynapSE: fast and slow excitatory as well as fast and slow inhibitory. Each neuron can receive inputs through up to 64 synapses, each of which can be any of the given types. Via `cortexcontrol` the synaptic behavior can be adjusted for each type and for each core, but not for individual synapses.\n",
    "\n",
    "There is a priori no difference between slow and fast excitatory synapses, so they can be set to have the same behavior. In fact, one could assign shorter time constants to the \"slow\" excitatory synapses, making them effectively the fast ones. While both excitatory and the fast inhibitory synapses work by adding or subtracting current to the neuron membrane, the slow inhibitory synapses use shunt inhibition and in practice silence a neuron very quickly.\n",
    "\n",
    "Note that all synapses that are of the same type and that are on the same core have the same weight. Different connection strengths between neurons can only be achieved by setting the same connection multiple times. Therefore the weight matrices `weights_in` and `weights_rec` can only be positive or negative integers and thus determine the number of excitatory and inhibitory connections to a neuron.\n",
    "\n",
    "In this layer the connections from external input to layer neurons are fast excitatory or inhibitory synapses. Outgoing connections from neurons that receive external input are fast excitatory or inhibitory. Outgoing connecitons from other neurons are slow excitatory and fast inhibitory. For different configurations the `_compile_weights_and_configure` method has to be modified.  Felix can help you with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The `.evolve` method takes the standard arguments `ts_input`, `duration`, `num_timesteps` and `verbose`, which is currently not being used. Evolution duration is determined by the usual rules. If `ts_input` is provided, it must be a `.TSEvent` object.\n",
    "\n",
    "The input is sent to the hardware neurons which will evolve and spike. The neurons' spikes are collected in a `.TSEvent` object with the timings and IDs of the spiking neurons. \n",
    "\n",
    "Note that the hardware neurons continue evolving, so in contrast to software simulations, the layer will be in a different state (membrane potentials etc.) when an evolution begins than when the previous evolution ended. Because evolution happens in batches, this even happens between some of the timesteps within the evolution (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution in batches\n",
    "As for now it is not possible to stream events (input spikes) continuously to the DynapSE. Therefore a group of events is transferred to the hardware, temporarily stored there and then translated to spikes of virtual neurons, with temporal order and inter-spike intervals matching the input signal.\n",
    "\n",
    "The number of events that can be sent at once is limited. To allow for arbitrarily long layer evolution times, the input can be split into batches, during each of which a number of events is sent and \"played back\" to the hardware."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "There are two ways of splitting the full input signal into batches. First, a new batch ends, as soon as it contains the maximum number of spikes or lasts the maximum allowed batch duration. These values can be set for each `.RecDynapSE` object and are stored in the `max_num_events_batch` and `max_batch_dur` attributes. While the former is limited by the hardware to be at most $2^{16}-1 = 65535$, the latter can be set to `None`, which in principle allows for arbitrarily long batch durations. However, the length of any inter-spike interval is currently limited, too, so effectively the batch duration is limited also in this case (see *Choosing dt* for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the hardware neurons keep evolving after a batch ends, their state (membrane potentials etc.) will have changed until the next batch starts. These discontinuities could be problematic for some simulations. In particular if the input data consists of trials, no trial should be divided over two batches.\n",
    "\n",
    "The second way of splitting batches considers this scenario and by splitting the input only at the beginnings of trials. The number of trials in a batch is determined by the layer's `nMaxTrialPerBatch` attribute. If a batch with this number of trials contains more events or lasts longer than allowed, the number of trials is reduced accordingly. This method is used if the input time series has a `trial_start_times` attribute, that indicates the start time of each trial, and if `max_num_trials_batch` is not `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "As usual the layer's time and state can be reset by the `.RecDynapSE.reset_time`, `.RecDynapSE.reset_state` and `.RecDynapSE.reset_all` methods. However, since there is no state vector in this class, `.RecDynapSE.reset_state` has no effect and `.RecDynapSE.reset_all` effectively does the same as `.RecDynapSE.reset_time`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal methods\n",
    "\n",
    "```\n",
    "_batch_input_data(\n",
    "    self, ts_input: TSEvent, num_timesteps: int, verbose: bool = False\n",
    ") -> (np.ndarray, int)\n",
    "```\n",
    "This mehtod is called by evolve, which passes it the evolution input `ts_input` and the number of evolution timesteps `num_timesteps`. It splits the input into batches according to the maximum duration, number of events and number of trials and returns a generator that for each batch yields the timesteps and channels of the input events in the batch, the time step at which the batch begins and teh duration of the batch.\n",
    "\n",
    "```\n",
    "_compile_weights_and_configure(self)\n",
    "```\n",
    "Configures the synaptic connections on the hardware according to the layer weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class member overview\n",
    "\n",
    "### Methods\n",
    "\n",
    "| arguments: | Description |\n",
    "|--------|-------------|\n",
    "| `_batch_input_data` | Split evolution into batches and return generator |\n",
    "| `_compile_weights_and_configure` | Configure hardware synaptic connections |\n",
    "| `evolve` | Evolve layer |\n",
    "| `reset_all` | Reset layer time to 0|\n",
    "| `reset_state` | Do nothing. |\n",
    "| `reset_time` | Reset layer time to 0 |\n",
    "\n",
    "*Internal methods of parent class* `Layer` *are listed in corresponding documentation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "Each argument that described in section Instantiation has a corresponding attribute that can be accessed by `<Layer>.<attribute>`, where `<Layer>` is the layer instance and `<attribute>` the argument name. Furthermore there are a few internal attributes:\n",
    "\n",
    "| Attribute name | Description |\n",
    "|----------------|-------------|\n",
    "| `_vHWNeurons` | 1D-Array of hardware neurons used for the layer. |\n",
    "| `vVirtualNeurons` | 1D-Array of virtual neurons used for the layer. |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "430px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
