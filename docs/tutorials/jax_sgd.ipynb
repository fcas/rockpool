{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent training of a rate-based reservoir model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This tutorial demonstrates using |project| and a ``Jax``-accelerated rate-based reservoir layer to perform gradient descent training of all network parameters. The result is a trained dynamic recurrent network with long memory, optimised to perform a signal generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This example requires the |project| package from aiCTX, as well as ``jax`` and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'rockpool.layers.training.gpl.train_jax_rate_sgd'\n",
      "No module named 'rpyc'\n",
      "No module named 'rpyc'\n",
      "No module named 'rpyc'\n",
      "cannot import name 'JaxTrainedLayer'\n"
     ]
    }
   ],
   "source": [
    "# - Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# - Imports and boilerplate \n",
    "from rockpool import TimeSeries, TSContinuous\n",
    "from rockpool.layers import RecRateEulerJax_IO, H_ReLU, H_tanh\n",
    "from tqdm import tnrange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggered signal-generation task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will use a pulse-to-chirp task as a demonstration. The reservoir receives a short pulse, and must respond by generating a chirp signal (a sinusoid increasing in frequency over time). This task is difficult, as no input is present for most of the time, and so considerable reservoir memory is required.\n",
    "\n",
    "You can adjust the parameters of the input by changing the number of repeats ``num_repeats``, the duration of the input pulse ``pulse_duration``, and the maximum frequency reached by the chirp ``chirp_freq_factor``. Shorter pulses and higher chirp frequencies make the problem more difficult. More repeats make learning more difficult, by forcing gradients to be accumulated over more time steps.\n",
    "\n",
    "You can also adjust the time step ``dt``, which makes learning slower (more time points evaluated per trial), but which permits shorter time constants to be used in the network. For numerical stability, time constants must be at least ``10*dt``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# - Define input and target signals \n",
    "num_repeats = 1\n",
    "pulse_duration = 50e-3\n",
    "chirp_freq_factor = 10\n",
    "padding_duration = 1\n",
    "chirp_duration = 1\n",
    "dt = 1e-3\n",
    "\n",
    "# - Build chirp and trigger signals\n",
    "chirp_end = int(np.round(chirp_duration / dt))\n",
    "chirp_timebase = np.linspace(0, chirp_end * dt, chirp_end)\n",
    "chirp = np.atleast_2d(np.sin(chirp_timebase * 2 * np.pi * (chirp_timebase * chirp_freq_factor))).T\n",
    "trigger = np.atleast_2d(chirp_timebase < pulse_duration).T\n",
    "\n",
    "# - Add padding\n",
    "padding = np.zeros((int(np.round(padding_duration / dt)), 1))\n",
    "chirp = np.vstack((padding, chirp, padding))\n",
    "trigger = np.vstack((padding, trigger, padding))\n",
    "\n",
    "# - Build a time base\n",
    "num_samples = (chirp_end + len(padding)*2) * num_repeats\n",
    "timebase = np.linspace(0, num_samples, num_samples + 1)\n",
    "timebase = timebase[:-1] * dt\n",
    "\n",
    "# - Replicate out inputs and target signals\n",
    "input_t = np.tile(trigger * 1., (num_repeats, 1))\n",
    "target_t = np.tile(chirp, (num_repeats, 1))\n",
    "\n",
    "# - Generate time series objects\n",
    "ts_input = TSContinuous(timebase, input_t, periodic=True)\n",
    "ts_target = TSContinuous(timebase, target_t, periodic=True)\n",
    "\n",
    "# - Plot the input and target signals\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "     timebase,\n",
    "     input_t,\n",
    "     timebase,\n",
    "     target_t,\n",
    "     # timebase, target_t + np.diff(np.vstack((target_t, target_t[0])), axis=0) / dt * tau,\n",
    " )\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Input / target amplitude')\n",
    "plt.legend((\"Input\", \"Target\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will define a ReLU reservoir, with a single input and a single output channel, and with a chosen number of units in the recurrent layer ``nResSize``. Larger reservoirs take longer to train, but perform the task to higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of a unit $j$ in the recurrent layer is given by\n",
    "\n",
    "$$\n",
    "\\tau_j\\frac{\\textrm{d}{x_j}}{\\textrm{d}t} + {x_j} = W_r \\cdot f(\\textbf{x}) + b_j + i_j + \\sigma\\zeta_j(t)\n",
    "$$\n",
    "\n",
    "where $\\tau_j$ is the time constant of the unit (`tau`); $W_r$ is the $N \\times N$ weight matrix defining the recurrent connections; $\\textbf{x}$ is the vector of recurrent layer activities (`w_rec`); $f(x)$ is the neuron transfer function $\\tanh(x)$; $b_j$ is the bias input of the unit (`bias`); $i_j$ is the external input to the unit; and $\\sigma\\zeta_j(t)$ is a white noise process with standard deviation $\\sigma$ (`noise_std`).\n",
    "\n",
    "External input is weighted such that $\\textbf{i} = W_i \\cdot i(t)$, where $W_i$ is the external input weight matrix (`w_in`) and $i(t)$ is the external input function.\n",
    "\n",
    "The output of the reservoir is also weighted such that $z = W_o \\cdot \\textbf{x}$, where $W_o$ is the output weight matrix (`w_out`). The goal of the training task is to match the reservoir output $\\hat{z}$ with a target signal $z^*$.\n",
    "\n",
    "Weight initialisation doesn't seem to matter too much in this process; gradient descent does a good job even when weights are initially zero. Here we use a standard initialisation with unit spectral radius for the recurrent weights.\n",
    "\n",
    "You can change the activation function to one of `H_tanh` or `H_ReLU`. You can also define your own, but must use `jax.numpy` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define the reservoir parameters\n",
    "nResSize = 100\n",
    "tau = 20e-3\n",
    "bias = 0.\n",
    "activation_func = H_ReLU\n",
    "noise_std = 0.1\n",
    "\n",
    "# - Build a layer\n",
    "nInSize = 1\n",
    "nOutSize = 1\n",
    "w_in = 2*npr.rand(nInSize, nResSize)-1\n",
    "w_rec = npr.randn(nResSize, nResSize) / np.sqrt(nResSize)\n",
    "w_out = 2*npr.rand(nResSize, nOutSize)-1\n",
    "\n",
    "lyrRes = RecRateEulerJax_IO(w_in, w_rec, w_out, tau, bias,\n",
    "                            dt = dt, noise_std = noise_std,\n",
    "                            activation_func = activation_func)\n",
    "\n",
    "# - Get initial output\n",
    "ts_output0 = lyrRes.evolve(ts_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Force initialisation of training\n",
    "l, g = lyrRes.train_output_target(ts_input, ts_target, is_first = True);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Here we use the training method :py:meth:`.train_output_target` to perform stochastic descent using the Adam optimiser. You can re-run the cell below as many times as you like if you want to extend the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is a combination of mean-squared-error between the reservoir output and the target signal $\\frac{1}{T} (\\hat{z} - z^*)^2$; a factor that harshly penalises time constants $\\tau$ shorter than the minimum time constant $\\tau_{min}$; and a factor related to the 2-norm of the recurrent weight matrix `np.mean(w_recurrent ** 2)` or $||W_r||^2$. This helps to ensure that the network remains stable.\n",
    "\n",
    "At the end of each batch the network is testing by providing a pulse input, and plotting the reservoir response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     15
    ]
   },
   "outputs": [],
   "source": [
    "num_batches = 10\n",
    "trials_per_batch = 200\n",
    "\n",
    "def two_norm(params):\n",
    "    return np.sqrt(np.sum([np.sum(e ** 2) for e in params.values()]))\n",
    "\n",
    "with tnrange(num_batches, desc='batches') as tqdm_batches:\n",
    "    for _ in range(num_batches):\n",
    "        with tnrange(trials_per_batch, desc='trials', leave=False) as tqdm_trials:\n",
    "            for _ in range(trials_per_batch):\n",
    "                # - Get this trial\n",
    "                pass\n",
    "\n",
    "                # - One step of Adam training\n",
    "                loss, grad_loss = lyrRes.train_output_target(ts_input, ts_target)\n",
    "\n",
    "                # - Update statistics\n",
    "                tqdm_trials.set_postfix(loss=loss(),\n",
    "                                        #norm_g=two_norm(grad_loss()),\n",
    "                                        min_tau=int(np.min(lyrRes.tau) / 1e-3),\n",
    "                                        refresh=False)\n",
    "                tqdm_trials.update()\n",
    "            \n",
    "            # - Update batch progress\n",
    "            tqdm_batches.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     15
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ts_output = lyrRes.evolve(ts_input)\n",
    "plt.plot(ts_output.times, ts_target(ts_output.times))\n",
    "ts_output.plot(ls = '--')\n",
    "plt.legend(['Target', 'Output'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Output / target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone according to plan, the output of the reservoir should closely match the target signal. We can see the effect of training by examining the distribution of network parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the network time constants\n",
    "plt.figure()\n",
    "plt.hist(lyrRes.tau / 1e-3, 21);\n",
    "plt.stem([tau / 1e-3], [50], 'r-')\n",
    "plt.legend(('Trained time constants', 'Initial time constants'))\n",
    "plt.xlabel('Time constant (ms)');\n",
    "plt.ylabel('Count');\n",
    "plt.title('Distribution of time constants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot the recurrent layer biases\n",
    "plt.figure()\n",
    "plt.stem(lyrRes.bias);\n",
    "plt.title('Distribution of trained biases')\n",
    "plt.xlabel('Unit')\n",
    "plt.ylabel('Bias');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine something of the computational properties of the network by finding the eigenspectrum of the Jacobian of the recurrent layer. The Jacobian $\\hat{J}$ is given by\n",
    "\n",
    "$$\n",
    "\\hat{J} = (\\hat{W_r} - I) ./ \\hat{T}\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix, $./$ denotes element-wise division, and $T$ is the matrix composed of all time constants $\\hat{\\tau}$ of the recurrent layer.\n",
    "\n",
    "Below we plot the eigenvalues $\\lambda$ of $J$. In my training result, several complex eigenvalues $\\lambda$ with real parts greater than zero are present in the trained eigenspectrum. These correspond to oscillatory modes, which are obviously useful in generating the chirp output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# - Plot the recurrent layer eigenspectrum \n",
    "J = lyrRes.w_recurrent - np.identity(nResSize)\n",
    "J = J / lyrRes.tau\n",
    "\n",
    "J0 = w_rec - np.identity(nResSize)\n",
    "J0 = J0 / tau\n",
    "\n",
    "plt.figure()\n",
    "eigs = np.linalg.eigvals(J)\n",
    "eigs0 = np.linalg.eigvals(J0)\n",
    "plt.plot(np.real(eigs0), np.imag(eigs0), '.')\n",
    "plt.plot(np.real(eigs), np.imag(eigs), 'o')\n",
    "plt.plot([0, 0], [-100, 100], '--')\n",
    "plt.legend(('Initial eigenspectrum', 'Trained eigenspectrum',))\n",
    "plt.xlim([-350, 250])\n",
    "plt.ylim([-100, 100])\n",
    "plt.title('Eigenspectrum of recurrent weights')\n",
    "plt.xlabel('Re($\\lambda$)')\n",
    "plt.ylabel('Im($\\lambda$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Gradient descent training does a good job of optimmising a dynamic recurrent network for a difficult task requiring significant reservoir memory. `jax` provides a computationally efficient back-end as well as automatic differentiation of the recurrent reservoir layer."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:py36_tf]",
   "language": "python",
   "name": "conda-env-py36_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
