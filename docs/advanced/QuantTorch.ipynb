{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19b6e6e",
   "metadata": {},
   "source": [
    "# Torch transformation-in-training pipeline prototype\n",
    "This notebook gives an overview of the prototype parameter and activation quantization-aware-training pipeline and facilities available for Torch-backed modules in Rockpool.\n",
    "\n",
    "This is still work-in-progress and subject to change.\n",
    "\n",
    "The torch pipeline is based on Torch's `functional_call` API, new in Torch 1.12.\n",
    "\n",
    "## Design goals\n",
    "* No need to modify pre-defined modules to make \"magic quantization\" modules\n",
    "* General solution that can be applied widely to modules and parameters\n",
    "* Convenient API for specifying transformations over parameters in a network in a \"grouped\" way, using Rockpool's parameter families\n",
    "* Similar API for parameter- and activity-transformation\n",
    "* Quantization controllable at a fine-grained level\n",
    "* Provide useful and flexible transformation methods --- can be used for QAT, dropout, pruning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4967ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Basic imports\n",
    "from rockpool.nn.modules import LinearTorch, LIFTorch\n",
    "from rockpool.nn.combinators import Sequential, Residual\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec26e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Transformation pipeline imports\n",
    "import rockpool.transform.torch_transform as tt\n",
    "import rockpool.utilities.tree_utils as tu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc70ce6",
   "metadata": {},
   "source": [
    "## Parameter transformations\n",
    "The parameter transformation pipeline allows you to insert transformations to any parameter in the forward pass before evolution, in a configurable way. You would use this to perform quantisation-aware-training, random parameter attacks, connection pruning, ...\n",
    "\n",
    "We'll begin here with a simple Rockpool SNN that uses most of the features of network composition in Rockpool, and is compatible with Xylo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2631ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylan/SynSense Dropbox/Dylan Muir/LiveSync/Development/rockpool_GIT/rockpool/parameters.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = tensor(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    LinearTorch '0_LinearTorch' with shape (3, 5)\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        LinearTorch '0_LinearTorch' with shape (5, 5)\n",
       "        LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    }\n",
       "    LinearTorch '3_LinearTorch' with shape (5, 3)\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Build a network to use\n",
    "net = Sequential(\n",
    "    LinearTorch((3, 5)),\n",
    "    LIFTorch(5),\n",
    "    Residual(\n",
    "        LinearTorch((5, 5)),\n",
    "        LIFTorch(5, has_rec = True),\n",
    "    ),\n",
    "    LinearTorch((5, 3)),\n",
    "    LIFTorch(3),\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd9032",
   "metadata": {},
   "source": [
    "Now we build a configuration that describes the desired parameter transformation to apply to each parameter. We will transform weights with :py:func:`.stochastic_rounding` and transform biases with :py:func:`.dropout`. We can use parameter families to select the parameters to transform and which transformation to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0c6695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_LinearTorch': {'weight': <function __main__.<lambda>(p)>},\n",
       " '2_TorchResidual': {'0_LinearTorch': {'weight': <function __main__.<lambda>(p)>},\n",
       "  '1_LIFTorch': {'w_rec': <function __main__.<lambda>(p)>}},\n",
       " '3_LinearTorch': {'weight': <function __main__.<lambda>(p)>}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Get the 'weights' parameter family, and specify stochastic rounding\n",
    "tconfig = tt.make_param_T_config(net, lambda p: tt.stochastic_rounding(p, num_levels=2**2), 'weights')\n",
    "tconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1fb7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_LinearTorch': {'weight': <function __main__.<lambda>(p)>},\n",
       " '2_TorchResidual': {'0_LinearTorch': {'weight': <function __main__.<lambda>(p)>},\n",
       "  '1_LIFTorch': {'w_rec': <function __main__.<lambda>(p)>}},\n",
       " '3_LinearTorch': {'weight': <function __main__.<lambda>(p)>}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Now we add in the bias transformation\n",
    "tu.tree_update(tconfig, tt.make_param_T_config(net, lambda p: tt.dropout(p, 0.3), 'biases'))\n",
    "tconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e511f",
   "metadata": {},
   "source": [
    "We then use this quantization configuration tree to patch the network with transformation modules, with the :py:func:`.make_param_T_network` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82edc75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    TWrapper '0_LinearTorch' with shape (3, 5) {\n",
       "        LinearTorch '_mod' with shape (3, 5)\n",
       "    }\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        TWrapper '0_LinearTorch' with shape (5, 5) {\n",
       "            LinearTorch '_mod' with shape (5, 5)\n",
       "        }\n",
       "        TWrapper '1_LIFTorch' with shape (5, 5) {\n",
       "            LIFTorch '_mod' with shape (5, 5)\n",
       "        }\n",
       "    }\n",
       "    TWrapper '3_LinearTorch' with shape (5, 3) {\n",
       "        LinearTorch '_mod' with shape (5, 3)\n",
       "    }\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - We now use this configuration to patch the original network with transformation modules\n",
    "tnet = tt.make_param_T_network(net, tconfig)\n",
    "tnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fee40b",
   "metadata": {},
   "source": [
    "Each of the transformed modules is now wrapped in a :py:class:`.TWrapper` module --- these special wrapper modules apply any required transformations to the wrapped module, in the forward pass, injecting the transformed parameters and then evolving the wrapped module as usual. The original module doesn't need to know anything special, and simply uses the quantized parameters passed to it.\n",
    "\n",
    "The parameters are held by the original modules, un-transofmred, so that any parameters updates during training are applied to the un-transformed parameters.\n",
    "\n",
    "If we investigate the :py:meth:`.Module.parameters` of the network we can see this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425e17e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "   tensor([[-1.1399e+00,  1.4406e-01,  2.5429e-01, -9.5625e-01,  5.6410e-01],\n",
       "           [-8.6451e-04, -7.1668e-01, -6.2177e-02, -1.2791e+00,  1.2483e+00],\n",
       "           [-9.4208e-01,  3.3010e-01,  5.8619e-01, -4.7097e-01, -3.7961e-02]],\n",
       "          requires_grad=True)}},\n",
       " '2_TorchResidual': {'0_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "    tensor([[ 0.3838,  0.9660,  0.9430,  0.4541, -0.1916],\n",
       "            [ 0.9990, -0.7228,  0.7537, -0.3913,  0.3117],\n",
       "            [ 0.6027,  0.7698, -1.0840, -0.3215,  0.5501],\n",
       "            [ 0.8245, -0.2296, -0.1558,  0.0450,  0.2370],\n",
       "            [-0.3791,  0.9442, -0.8692,  0.7645,  0.5853]], requires_grad=True)}},\n",
       "  '1_LIFTorch': {'_mod': {'w_rec': Parameter containing:\n",
       "    tensor([[ 0.1149, -0.6803,  0.2665,  1.0365,  0.5285],\n",
       "            [ 0.5471,  0.5533,  0.8679, -0.9686, -0.9631],\n",
       "            [-0.5324, -0.6320,  0.8431, -0.9942, -1.0700],\n",
       "            [ 0.2207, -0.7819,  0.6502,  0.1557,  1.0289],\n",
       "            [ 0.1045,  0.1314, -0.7984, -0.3872, -0.0130]], requires_grad=True)}}},\n",
       " '3_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "   tensor([[ 0.0664,  0.0429, -1.0104],\n",
       "           [-0.8827,  0.3947, -0.9613],\n",
       "           [ 0.4150, -0.1393,  0.5863],\n",
       "           [-0.2530,  0.5339, -0.6595],\n",
       "           [-0.1474, -0.5120,  0.4852]], requires_grad=True)}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnet.parameters('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e82835",
   "metadata": {},
   "source": [
    "These are the un-transformed parameters, in floating-point format. But if we evolve the module by calling it, the parameters will all be transformed in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4cd0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0., -0.,  0.],\n",
       "         [-0.,  0., -0.],\n",
       "         [-0.,  4., -0.],\n",
       "         [-0., 10., -0.],\n",
       "         [-0., 18., -0.],\n",
       "         [-0., 23., -0.],\n",
       "         [-0., 24., -0.],\n",
       "         [-0., 22., -0.],\n",
       "         [-0., 16., -0.],\n",
       "         [-0.,  6., -0.]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, ns, rd = tnet(torch.ones(1, 10, 3))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6847c43",
   "metadata": {},
   "source": [
    "**Training goes here!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ab09a",
   "metadata": {},
   "source": [
    "\n",
    "Here you can train the model, interacting with it as any other Rockpool :py:class:`.TorchModule`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee08f5d",
   "metadata": {},
   "source": [
    "Once we've trained the model, you might want to access the transformed parameters. At this point you have two options:\n",
    "\n",
    "1. you can execute the transformation such that the parameters are updated manually, using the helper function :py:func:`.apply_T`. This will \"burn in\" the transformation, storing the result as the \"real\" parameters within the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe66d815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    ActWrapper '0_LinearTorch' with shape (3, 5) {\n",
       "        LinearTorch '_mod' with shape (3, 5)\n",
       "    }\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        ActWrapper '0_LinearTorch' with shape (5, 5) {\n",
       "            LinearTorch '_mod' with shape (5, 5)\n",
       "        }\n",
       "        LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    }\n",
       "    ActWrapper '3_LinearTorch' with shape (5, 3) {\n",
       "        LinearTorch '_mod' with shape (5, 3)\n",
       "    }\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttnet = tt.apply_T(tnet)\n",
    "ttnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291f56a",
   "metadata": {},
   "source": [
    "If we now examine the parameters, we will see the low-resolution quantized versions (still stored as floating-point numbers -- this transformation did not force the parameters to be integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a53b5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "   tensor([[-1.2824,  1.1413,  0.0877,  0.0578, -1.0581],\n",
       "           [ 0.5104,  1.1349, -1.3793, -0.1282,  0.2503],\n",
       "           [-0.3207, -0.3518,  0.9708, -0.0555, -1.1052]], requires_grad=True)}},\n",
       " '2_TorchResidual': {'0_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "    tensor([[-0.0842,  0.2129, -0.2631, -0.6283,  0.1849],\n",
       "            [-0.4626, -0.7207,  0.1398,  0.2352, -0.2340],\n",
       "            [-0.0220, -0.5523,  0.2323, -0.2215, -0.1106],\n",
       "            [-0.9892,  0.2199, -0.2059,  0.4504, -0.6871],\n",
       "            [-0.5974, -0.9785,  0.1854, -0.0626, -0.7767]], requires_grad=True)}},\n",
       "  '1_LIFTorch': {'w_rec': Parameter containing:\n",
       "   tensor([[ 0.6062, -0.9756, -0.3870, -0.6646,  0.0906],\n",
       "           [-0.7449, -0.8844,  0.0129, -0.9246,  0.9288],\n",
       "           [ 0.0158,  0.1328,  0.6948,  0.9710, -0.2518],\n",
       "           [ 0.3792, -0.4621,  0.8133, -0.4422, -0.1458],\n",
       "           [-0.3411,  0.4592, -0.0581,  0.3192,  1.0715]], requires_grad=True)}},\n",
       " '3_LinearTorch': {'_mod': {'weight': Parameter containing:\n",
       "   tensor([[-0.8873, -0.2453,  0.3578],\n",
       "           [-0.4203, -0.6405,  0.4401],\n",
       "           [ 0.7149, -0.6588, -0.9909],\n",
       "           [-0.0869,  0.4223,  0.9218],\n",
       "           [-0.5677,  0.4923, -0.5110]], requires_grad=True)}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttnet.parameters('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17318a1",
   "metadata": {},
   "source": [
    "You can now convert the network back to the original \"unpatched\" structure with the helper function  :py:func:`.remove_T_net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b19ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    LinearTorch '0_LinearTorch' with shape (3, 5)\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        LinearTorch '0_LinearTorch' with shape (5, 5)\n",
       "        LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    }\n",
       "    LinearTorch '3_LinearTorch' with shape (5, 3)\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpatched_net = tt.remove_T_net(ttnet)\n",
    "unpatched_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4855d3",
   "metadata": {},
   "source": [
    "Compare this with the original network above.\n",
    "\n",
    "2. The second option is to \"unpatch\" the network with :py:func:`.remove_T_net` and use post-training quantisation through whatever method you prefer. This might be preferable if you have included \"destructive\" transformations such as :py:func:`.dropout`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec794a",
   "metadata": {},
   "source": [
    "### How to: Quantize to round numbers\n",
    "We might want to quantize to integer levels, for example when targetting processors that use integer logic and representations for parameters (such as Xylo). This is possible with :py:func:`.stochastic_rounding`.\n",
    "\n",
    "The cell below shows you how to use :py:func:`.stochastic_rounding` to target signed integer parameter values. By default, :py:func:`.stochastic_rounding` makes sure that zero in the input space maps to a zero in the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "330e6267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -5., -1.,  6.,  6.],\n",
       "        [ 1., -1., -3.,  3.,  1.],\n",
       "        [ 3., -5.,  4., -3.,  8.],\n",
       "        [-3.,  7.,  5.,  3.,  6.],\n",
       "        [ 5., -7.,  4., -7.,  1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand((5, 5)) - 0.5\n",
    "\n",
    "num_bits = 4\n",
    "\n",
    "tt.stochastic_rounding(\n",
    "    w,\n",
    "    output_range=[-2**(num_bits-1)+1, 2**(num_bits-1)],\n",
    "    num_levels=2**num_bits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88306e9",
   "metadata": {},
   "source": [
    "## Activity transformations\n",
    "There is a similar pipeline available for activity transformations. This can be used to transform the output of modules in the forward pass, without modifying the module code.\n",
    "\n",
    "Let's begin again with a simple SNN artchitecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "360c1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylan/SynSense Dropbox/Dylan Muir/LiveSync/Development/rockpool_GIT/rockpool/parameters.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = tensor(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    LinearTorch '0_LinearTorch' with shape (3, 5)\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        LinearTorch '0_LinearTorch' with shape (5, 5)\n",
       "        LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    }\n",
       "    LinearTorch '3_LinearTorch' with shape (5, 3)\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Build a network to use\n",
    "net = Sequential(\n",
    "    LinearTorch((3, 5)),\n",
    "    LIFTorch(5),\n",
    "    Residual(\n",
    "        LinearTorch((5, 5)),\n",
    "        LIFTorch(5, has_rec = True),\n",
    "    ),\n",
    "    LinearTorch((5, 3)),\n",
    "    LIFTorch(3),\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e7254",
   "metadata": {},
   "source": [
    "We need to build a configuration to patch the network with. We can conveniently specify which modules to transform according to the module class. Here we'll perform rounding of output activations to 8-bit signed integers, using the function :py:func:`.deterministic_rounding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6397b403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': None,\n",
       " '0_LinearTorch': {'': <function __main__.<lambda>(p)>},\n",
       " '1_LIFTorch': {'': None},\n",
       " '2_TorchResidual': {'': None,\n",
       "  '0_LinearTorch': {'': <function __main__.<lambda>(p)>},\n",
       "  '1_LIFTorch': {'': None}},\n",
       " '3_LinearTorch': {'': <function __main__.<lambda>(p)>},\n",
       " '4_LIFTorch': {'': None}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Build a null configuration tree, which can be manipulated directly\n",
    "tt.make_act_T_config(net)\n",
    "\n",
    "# - Specify a transformation function as a lambda\n",
    "T_fn = lambda p: tt.deterministic_rounding(p, output_range = [-128, 127], num_levels = 2**8)\n",
    "\n",
    "# - Conveniently build a configuration tree by selecting a module class\n",
    "tconf = tt.make_act_T_config(net, T_fn, LinearTorch)\n",
    "tconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32cc34",
   "metadata": {},
   "source": [
    "Now we patch the network, analogously to the parameter transformation above, using the helper function :py:func:`.make_act_T_network`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc168fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (3, 3) {\n",
       "    ActWrapper '0_LinearTorch' with shape (3, 5) {\n",
       "        LinearTorch '_mod' with shape (3, 5)\n",
       "    }\n",
       "    LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    TorchResidual '2_TorchResidual' with shape (5, 5) {\n",
       "        ActWrapper '0_LinearTorch' with shape (5, 5) {\n",
       "            LinearTorch '_mod' with shape (5, 5)\n",
       "        }\n",
       "        LIFTorch '1_LIFTorch' with shape (5, 5)\n",
       "    }\n",
       "    ActWrapper '3_LinearTorch' with shape (5, 3) {\n",
       "        LinearTorch '_mod' with shape (5, 3)\n",
       "    }\n",
       "    LIFTorch '4_LIFTorch' with shape (3, 3)\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Make a transformed network by patching with the configuration\n",
    "tnet = tt.make_act_T_network(net, tconf)\n",
    "tnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5ec60",
   "metadata": {},
   "source": [
    "Again, the network has been patched (this time with :py:class:`.ActWrapper` modules), each of which handle the transformations for a single wrapped module.\n",
    "\n",
    "Now we evolve the module as useful, and check the outputs of the :py:clas:`LinearTorch` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbfdc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - We evolve the module as usual\n",
    "out, ns, rd = tnet(torch.ones(1, 10, 3), record = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d55f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_LinearTorch': {},\n",
       " '0_LinearTorch_output': tensor([[[ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.],\n",
       "          [ -73.,  127.,  -22.,   -9., -127.]]], requires_grad=True),\n",
       " '1_LIFTorch': {'vmem': tensor([[[-3.3144e+03,  5.1770e-01, -9.9886e+02, -4.0862e+02, -5.7661e+03],\n",
       "           [-3.7952e+03,  9.9487e-02, -1.1437e+03, -4.6790e+02, -6.6025e+03],\n",
       "           [-4.2906e+03,  1.4648e-03, -1.2931e+03, -5.2897e+02, -7.4644e+03],\n",
       "           [-4.7981e+03,  9.7461e-01, -1.4460e+03, -5.9154e+02, -8.3474e+03],\n",
       "           [-5.3153e+03,  8.9099e-01, -1.6019e+03, -6.5531e+02, -9.2472e+03],\n",
       "           [-5.8401e+03,  8.7622e-01, -1.7600e+03, -7.2002e+02, -1.0160e+04],\n",
       "           [-6.3706e+03,  1.4392e-01, -1.9199e+03, -7.8541e+02, -1.1083e+04],\n",
       "           [-6.9048e+03,  8.1665e-02, -2.0809e+03, -8.5128e+02, -1.2012e+04],\n",
       "           [-7.4412e+03,  1.3855e-01, -2.2426e+03, -9.1741e+02, -1.2946e+04],\n",
       "           [-7.9783e+03,  9.1345e-01, -2.4044e+03, -9.8363e+02, -1.3880e+04]]],\n",
       "         grad_fn=<CopySlices>),\n",
       "  'isyn': tensor([[[[ -602.3408],\n",
       "            [ 1047.9080],\n",
       "            [ -181.5273],\n",
       "            [  -74.2612],\n",
       "            [-1047.9080]],\n",
       "  \n",
       "           [[ -642.4040],\n",
       "            [ 1117.6071],\n",
       "            [ -193.6012],\n",
       "            [  -79.2005],\n",
       "            [-1117.6071]],\n",
       "  \n",
       "           [[ -680.5134],\n",
       "            [ 1183.9069],\n",
       "            [ -205.0862],\n",
       "            [  -83.8989],\n",
       "            [-1183.9069]],\n",
       "  \n",
       "           [[ -716.7641],\n",
       "            [ 1246.9733],\n",
       "            [ -216.0111],\n",
       "            [  -88.3682],\n",
       "            [-1246.9733]],\n",
       "  \n",
       "           [[ -751.2469],\n",
       "            [ 1306.9639],\n",
       "            [ -226.4032],\n",
       "            [  -92.6195],\n",
       "            [-1306.9639]],\n",
       "  \n",
       "           [[ -784.0479],\n",
       "            [ 1364.0287],\n",
       "            [ -236.2884],\n",
       "            [  -96.6634],\n",
       "            [-1364.0287]],\n",
       "  \n",
       "           [[ -815.2492],\n",
       "            [ 1418.3104],\n",
       "            [ -245.6915],\n",
       "            [ -100.5102],\n",
       "            [-1418.3104]],\n",
       "  \n",
       "           [[ -844.9288],\n",
       "            [ 1469.9448],\n",
       "            [ -254.6361],\n",
       "            [ -104.1693],\n",
       "            [-1469.9448]],\n",
       "  \n",
       "           [[ -873.1609],\n",
       "            [ 1519.0609],\n",
       "            [ -263.1443],\n",
       "            [ -107.6500],\n",
       "            [-1519.0609]],\n",
       "  \n",
       "           [[ -900.0162],\n",
       "            [ 1565.7816],\n",
       "            [ -271.2377],\n",
       "            [ -110.9609],\n",
       "            [-1565.7816]]]], grad_fn=<CopySlices>),\n",
       "  'spikes': tensor([[[  -0., 1048.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1118.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1184.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1246.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1307.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1364.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1419.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1470.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1519.,   -0.,   -0.,   -0.],\n",
       "           [  -0., 1565.,   -0.,   -0.,   -0.]]], grad_fn=<CopySlices>),\n",
       "  'irec': tensor([[[[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.],\n",
       "            [0.]]]]),\n",
       "  'U': tensor([[[0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.9816, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.5146, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.9968, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.9633, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.9961, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<CopySlices>)},\n",
       " '1_LIFTorch_output': tensor([[[  -0., 1048.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1118.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1184.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1246.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1307.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1364.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1419.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1470.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1519.,   -0.,   -0.,   -0.],\n",
       "          [  -0., 1565.,   -0.,   -0.,   -0.]]], requires_grad=True),\n",
       " '2_TorchResidual': {'0_LinearTorch': {},\n",
       "  '0_LinearTorch_output': tensor([[[ -55.,  -86.,   16.,   27.,  -28.],\n",
       "           [ -59.,  -92.,   17.,   29.,  -30.],\n",
       "           [ -62.,  -97.,   18.,   31.,  -32.],\n",
       "           [ -66., -102.,   19.,   33.,  -33.],\n",
       "           [ -69., -107.,   20.,   34.,  -35.],\n",
       "           [ -72., -112.,   21.,   36.,  -37.],\n",
       "           [ -75., -116.,   22.,   37.,  -38.],\n",
       "           [ -77., -120.,   23.,   39.,  -39.],\n",
       "           [ -80., -124.,   24.,   40.,  -41.],\n",
       "           [ -82., -128.,   24.,   41.,  -42.]]], requires_grad=True),\n",
       "  '1_LIFTorch': {'vmem': tensor([[[ 2.4561e-01, -1.0003e+04,  1.6797e-01,  3.3008e-01, -1.0943e+04],\n",
       "            [ 1.6406e-01, -2.1277e+04,  7.4219e-01,  7.3633e-01, -2.1254e+04],\n",
       "            [ 6.1328e-01, -4.6206e+04,  7.8125e-02,  9.9609e-01, -4.1022e+04],\n",
       "            [ 6.0547e-01, -9.9411e+04,  6.2500e-01,  4.6094e-01, -7.8480e+04],\n",
       "            [ 9.9219e-01, -2.0952e+05,  4.3750e-01,  8.5938e-01, -1.4866e+05],\n",
       "            [ 5.0000e-01, -4.3157e+05,  9.3750e-02,  5.0000e-01, -2.7871e+05],\n",
       "            [ 9.3750e-02, -8.6988e+05,  5.6250e-01,  7.8125e-01, -5.1723e+05],\n",
       "            [ 1.2500e-01, -1.7195e+06,  0.0000e+00,  6.2500e-01, -9.5041e+05],\n",
       "            [ 1.2500e-01, -3.3412e+06,  2.5000e-01,  0.0000e+00, -1.7296e+06],\n",
       "            [ 5.0000e-01, -6.3945e+06,  0.0000e+00,  5.0000e-01, -3.1184e+06]]],\n",
       "          grad_fn=<CopySlices>),\n",
       "   'isyn': tensor([[[[    4885.5767],\n",
       "             [   -5175.8291],\n",
       "             [   18670.9805],\n",
       "             [   10406.4648],\n",
       "             [   -5596.0791]],\n",
       "   \n",
       "            [[   11441.9307],\n",
       "             [  -11761.5977],\n",
       "             [   36369.5820],\n",
       "             [   19704.4219],\n",
       "             [  -10845.5859]],\n",
       "   \n",
       "            [[   25074.4570],\n",
       "             [  -25966.3047],\n",
       "             [   69682.3750],\n",
       "             [   36841.2969],\n",
       "             [  -20804.0273]],\n",
       "   \n",
       "            [[   52579.0234],\n",
       "             [  -55458.7891],\n",
       "             [  131629.5469],\n",
       "             [   68086.5156],\n",
       "             [  -39458.3594]],\n",
       "   \n",
       "            [[  106797.4141],\n",
       "             [ -114953.1719],\n",
       "             [  245546.8438],\n",
       "             [  124490.4219],\n",
       "             [  -74004.0547]],\n",
       "   \n",
       "            [[  211682.5625],\n",
       "             [ -232269.9219],\n",
       "             [  452878.6875],\n",
       "             [  225358.6875],\n",
       "             [ -137300.4062]],\n",
       "   \n",
       "            [[  411417.6250],\n",
       "             [ -459355.3125],\n",
       "             [  826558.5000],\n",
       "             [  404063.3125],\n",
       "             [ -252117.6875]],\n",
       "   \n",
       "            [[  786636.0625],\n",
       "             [ -892079.1875],\n",
       "             [ 1493718.5000],\n",
       "             [  717744.8750],\n",
       "             [ -458404.8438]],\n",
       "   \n",
       "            [[ 1483048.0000],\n",
       "             [-1705524.6250],\n",
       "             [ 2673843.2500],\n",
       "             [ 1263165.3750],\n",
       "             [ -825589.3125]],\n",
       "   \n",
       "            [[ 2761468.5000],\n",
       "             [-3216267.0000],\n",
       "             [ 4741994.0000],\n",
       "             [ 2202264.5000],\n",
       "             [-1473140.0000]]]], grad_fn=<CopySlices>),\n",
       "   'spikes': tensor([[[   4886.,      -0.,   18671.,   10407.,      -0.],\n",
       "            [  11442.,      -0.,   36369.,   19704.,      -0.],\n",
       "            [  25074.,      -0.,   69683.,   36841.,      -0.],\n",
       "            [  52579.,      -0.,  131629.,   68087.,      -0.],\n",
       "            [ 106797.,      -0.,  245547.,  124490.,      -0.],\n",
       "            [ 211683.,      -0.,  452879.,  225359.,      -0.],\n",
       "            [ 411418.,      -0.,  826558.,  404063.,      -0.],\n",
       "            [ 786636.,      -0., 1493719.,  717745.,      -0.],\n",
       "            [1483048.,      -0., 2673843., 1263166.,      -0.],\n",
       "            [2761468.,      -0., 4741994., 2202264.,      -0.]]],\n",
       "          grad_fn=<CopySlices>),\n",
       "   'irec': tensor([[[[    3333.6313],\n",
       "             [   -3075.6714],\n",
       "             [   10223.7695],\n",
       "             [    5479.5020],\n",
       "             [   -2987.7886]],\n",
       "   \n",
       "            [[    7201.9941],\n",
       "             [   -7096.7979],\n",
       "             [   19546.3105],\n",
       "             [   10279.2246],\n",
       "             [   -5775.5713]],\n",
       "   \n",
       "            [[   14980.1211],\n",
       "             [  -15439.0264],\n",
       "             [   36867.4766],\n",
       "             [   18994.7656],\n",
       "             [  -10993.0850]],\n",
       "   \n",
       "            [[   30266.3457],\n",
       "             [  -32233.9121],\n",
       "             [   68676.9531],\n",
       "             [   34703.0859],\n",
       "             [  -20644.4043]],\n",
       "   \n",
       "            [[   59763.0039],\n",
       "             [  -65281.1562],\n",
       "             [  126486.7500],\n",
       "             [   62752.6641],\n",
       "             [  -38304.9609]],\n",
       "   \n",
       "            [[  115810.3281],\n",
       "             [ -129113.4766],\n",
       "             [  230530.4062],\n",
       "             [  112386.6406],\n",
       "             [  -70298.8906]],\n",
       "   \n",
       "            [[  220903.8750],\n",
       "             [ -250521.0156],\n",
       "             [  416036.3750],\n",
       "             [  199384.3750],\n",
       "             [ -127705.6250]],\n",
       "   \n",
       "            [[  415627.1562],\n",
       "             [ -478341.7500],\n",
       "             [  743721.5000],\n",
       "             [  350442.1250],\n",
       "             [ -229751.0781]],\n",
       "   \n",
       "            [[  772529.5000],\n",
       "             [ -900765.6250],\n",
       "             [ 1317191.5000],\n",
       "             [  610144.3750],\n",
       "             [ -409472.2500]],\n",
       "   \n",
       "            [[ 1420085.8750],\n",
       "             [-1675515.8750],\n",
       "             [ 2311253.5000],\n",
       "             [ 1051970.7500],\n",
       "             [ -723038.1250]]]], grad_fn=<CopySlices>),\n",
       "   'U': tensor([[[0.9999, 0.0000, 0.9988, 1.0000, 0.0000],\n",
       "            [0.9986, 0.0000, 1.0000, 1.0000, 0.0000],\n",
       "            [1.0000, 0.0000, 0.9579, 1.0000, 0.0000],\n",
       "            [1.0000, 0.0000, 1.0000, 1.0000, 0.0000],\n",
       "            [1.0000, 0.0000, 1.0000, 1.0000, 0.0000],\n",
       "            [1.0000, 0.0000, 0.9770, 1.0000, 0.0000],\n",
       "            [0.9770, 0.0000, 1.0000, 1.0000, 0.0000],\n",
       "            [0.9933, 0.0000, 0.5000, 1.0000, 0.0000],\n",
       "            [0.9933, 0.0000, 1.0000, 0.5000, 0.0000],\n",
       "            [1.0000, 0.0000, 0.5000, 1.0000, 0.0000]]], grad_fn=<CopySlices>)},\n",
       "  '1_LIFTorch_output': tensor([[[   4886.,      -0.,   18671.,   10407.,      -0.],\n",
       "           [  11442.,      -0.,   36369.,   19704.,      -0.],\n",
       "           [  25074.,      -0.,   69683.,   36841.,      -0.],\n",
       "           [  52579.,      -0.,  131629.,   68087.,      -0.],\n",
       "           [ 106797.,      -0.,  245547.,  124490.,      -0.],\n",
       "           [ 211683.,      -0.,  452879.,  225359.,      -0.],\n",
       "           [ 411418.,      -0.,  826558.,  404063.,      -0.],\n",
       "           [ 786636.,      -0., 1493719.,  717745.,      -0.],\n",
       "           [1483048.,      -0., 2673843., 1263166.,      -0.],\n",
       "           [2761468.,      -0., 4741994., 2202264.,      -0.]]],\n",
       "         requires_grad=True)},\n",
       " '2_TorchResidual_output': tensor([[[4.8860e+03, 1.0480e+03, 1.8671e+04, 1.0407e+04, -0.0000e+00],\n",
       "          [1.1442e+04, 1.1180e+03, 3.6369e+04, 1.9704e+04, -0.0000e+00],\n",
       "          [2.5074e+04, 1.1840e+03, 6.9683e+04, 3.6841e+04, -0.0000e+00],\n",
       "          [5.2579e+04, 1.2460e+03, 1.3163e+05, 6.8087e+04, -0.0000e+00],\n",
       "          [1.0680e+05, 1.3070e+03, 2.4555e+05, 1.2449e+05, -0.0000e+00],\n",
       "          [2.1168e+05, 1.3640e+03, 4.5288e+05, 2.2536e+05, -0.0000e+00],\n",
       "          [4.1142e+05, 1.4190e+03, 8.2656e+05, 4.0406e+05, -0.0000e+00],\n",
       "          [7.8664e+05, 1.4700e+03, 1.4937e+06, 7.1774e+05, -0.0000e+00],\n",
       "          [1.4830e+06, 1.5190e+03, 2.6738e+06, 1.2632e+06, -0.0000e+00],\n",
       "          [2.7615e+06, 1.5650e+03, 4.7420e+06, 2.2023e+06, -0.0000e+00]]],\n",
       "        requires_grad=True),\n",
       " '3_LinearTorch': {},\n",
       " '3_LinearTorch_output': tensor([[[   0.,   -1.,   -1.],\n",
       "          [   0.,   -1.,   -1.],\n",
       "          [   1.,   -2.,   -2.],\n",
       "          [   1.,   -4.,   -3.],\n",
       "          [   3.,   -7.,   -4.],\n",
       "          [   5.,  -12.,   -8.],\n",
       "          [   8.,  -22.,  -14.],\n",
       "          [  13.,  -39.,  -24.],\n",
       "          [  21.,  -71.,  -43.],\n",
       "          [  33., -128.,  -75.]]], requires_grad=True),\n",
       " '4_LIFTorch': {'vmem': tensor([[[ 8.6050e-01, -8.6371e+02, -2.6507e+02],\n",
       "           [ 1.2563e-01, -1.0656e+03, -3.6021e+02],\n",
       "           [ 7.0586e-01, -1.2477e+03, -4.4734e+02],\n",
       "           [ 8.6485e-01, -1.4132e+03, -5.2797e+02],\n",
       "           [ 8.3739e-01, -1.5663e+03, -6.0348e+02],\n",
       "           [ 7.3882e-01, -1.7125e+03, -6.7798e+02],\n",
       "           [ 5.2736e-01, -1.8617e+03, -7.5709e+02],\n",
       "           [ 9.2163e-01, -2.0293e+03, -8.4970e+02],\n",
       "           [ 2.2905e-01, -2.2437e+03, -9.7238e+02],\n",
       "           [ 2.8667e-01, -2.5542e+03, -1.1524e+03]]], grad_fn=<CopySlices>),\n",
       "  'isyn': tensor([[[[ 165.3724],\n",
       "            [-255.5307],\n",
       "            [-112.6105]],\n",
       "  \n",
       "           [[ 157.3071],\n",
       "            [-244.0195],\n",
       "            [-108.0697]],\n",
       "  \n",
       "           [[ 150.5864],\n",
       "            [-234.0210],\n",
       "            [-104.7015]],\n",
       "  \n",
       "           [[ 144.1934],\n",
       "            [-226.4126],\n",
       "            [-102.4489]],\n",
       "  \n",
       "           [[ 140.0147],\n",
       "            [-222.0289],\n",
       "            [-101.2573]],\n",
       "  \n",
       "           [[ 137.9423],\n",
       "            [-222.6152],\n",
       "            [-103.9288]],\n",
       "  \n",
       "           [[ 138.8246],\n",
       "            [-232.6852],\n",
       "            [-112.1773]],\n",
       "  \n",
       "           [[ 144.4200],\n",
       "            [-258.4349],\n",
       "            [-129.5359]],\n",
       "  \n",
       "           [[ 157.3524],\n",
       "            [-313.3682],\n",
       "            [-164.1212]],\n",
       "  \n",
       "           [[ 181.0688],\n",
       "            [-419.8425],\n",
       "            [-227.4591]]]], grad_fn=<CopySlices>),\n",
       "  'spikes': tensor([[[165.,  -0.,  -0.],\n",
       "           [158.,  -0.,  -0.],\n",
       "           [150.,  -0.,  -0.],\n",
       "           [144.,  -0.,  -0.],\n",
       "           [140.,  -0.,  -0.],\n",
       "           [138.,  -0.,  -0.],\n",
       "           [139.,  -0.,  -0.],\n",
       "           [144.,  -0.,  -0.],\n",
       "           [158.,  -0.,  -0.],\n",
       "           [181.,  -0.,  -0.]]], grad_fn=<CopySlices>),\n",
       "  'irec': tensor([[[[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]],\n",
       "  \n",
       "           [[0.],\n",
       "            [0.],\n",
       "            [0.]]]]),\n",
       "  'U': tensor([[[1.0000, 0.0000, 0.0000],\n",
       "           [0.9935, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000],\n",
       "           [0.9999, 0.0000, 0.0000],\n",
       "           [1.0000, 0.0000, 0.0000]]], grad_fn=<CopySlices>)},\n",
       " '4_LIFTorch_output': tensor([[[165.,  -0.,  -0.],\n",
       "          [158.,  -0.,  -0.],\n",
       "          [150.,  -0.,  -0.],\n",
       "          [144.,  -0.,  -0.],\n",
       "          [140.,  -0.,  -0.],\n",
       "          [138.,  -0.,  -0.],\n",
       "          [139.,  -0.,  -0.],\n",
       "          [144.,  -0.,  -0.],\n",
       "          [158.,  -0.,  -0.],\n",
       "          [181.,  -0.,  -0.]]], requires_grad=True)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Examine the recorded outputs from the network; the LinearTorch layers have quantised output\n",
    "rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae87f9",
   "metadata": {},
   "source": [
    "As expected, the outputs of the Linear layers are now signed 8-bit integers, maintained as floating-point representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c0b9d47d5d3724c1090079e6a482fd3b557a4229bd09ca1206acef5b6a64516"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
