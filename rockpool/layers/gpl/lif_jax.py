##
# Spiking  layers with JAX backend
#

# - Imports
from ..layer import Layer
from ...timeseries import TSContinuous, TSEvent

from jax import numpy as np
import numpy as onp

from jax import jit, custom_gradient
from jax.lax import scan
import jax.random as rand

from typing import Optional, Tuple, Union
from warnings import warn

# - Define a float / array type
FloatVector = Union[float, np.ndarray]

# - Define module exports
__all__ = ["RecLIFJax",]

def _evolve_lif_jax(
        state0,
        w_in,
        w_rec,
        w_out_surrogate,
        tau_mem,
        tau_syn,
        bias,
        noise_std,
        sp_input_ts,
        I_input_ts,
        key,
        dt,
):
    # - Get evolution constants
    alpha = dt / tau_mem
    beta = np.exp(-dt / tau_syn)

    # - Surrogate functions to use in learning
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    @custom_gradient
    def step_pwl(x):
        s = np.clip(np.floor(x + 1.), 0.)
        return s, lambda g: (g * (x > -.5),)

    # - Single-step LIF dynamics
    def forward(state, inputs_t):
        # - Unpack inputs
        (sp_in_t, I_in_t) = inputs_t
        sp_in_t = sp_in_t.reshape(-1)
        Iin = I_in_t.reshape(-1)

        # - Synaptic input
        Irec = np.dot(state['spikes'], w_rec)
        dIsyn = sp_in_t + Irec
        state['Isyn'] = beta * state['Isyn'] + dIsyn

        # - Apply subtractive reset
        state['Vmem'] = state['Vmem'] - state['spikes']

        # - Membrane potentials
        dVmem = state['Isyn'] + Iin + bias - state['Vmem']
        state['Vmem'] = state['Vmem'] + alpha * dVmem

        # - Detect next spikes (with custom gradient)
        state['spikes'] = step_pwl(state['Vmem'])

        # - Return state and outputs
        return state, (Irec, state['spikes'], state['Vmem'], state['Isyn'])

    # - Generate membrane noise trace
    # - Build noise trace
    # - Compute random numbers for reservoir noise
    num_timesteps = np.max(I_input_ts.shape[0], sp_input_ts.shape[0])
    __all__, subkey = rand.split(key)
    noise_ts = noise_std * rand.normal(subkey, shape=(num_timesteps, np.size(state0['Vmem'])))

    # - Evolve over spiking inputs
    state, (Irec_ts, spikes_ts, Vmem_ts, Isyn_ts) = scan(
        forward,
        state0,
        (np.dot(sp_input_ts, w_in), I_input_ts + noise_ts)
    )

    # - Generate output surrogate
    surrogate_ts = sigmoid(Vmem_ts)

    # - Weighted output
    output_ts = np.dot(surrogate_ts, w_out_surrogate)

    # - Return outputs
    return state, Irec_ts, output_ts, surrogate_ts, spikes_ts, Vmem_ts, Isyn_ts

class RecLIFJax(Layer):
    """
    Recurrent spiking neuron layer (LIF), spiking input and spiking output. No input / output weights.

    `.RecLIFJax` is a basic recurrent spiking neuron layer, implemented with a JAX-backed Euler solver backend. Outputs are spikes generated by each layer neuron; no output weighting is provided. Inputs are provided by spiking through a synapse onto each layer neuron; no input weighting is provided. The layer is therefore N inputs -> N neurons -> N outputs.

    This layer can be used to implement gradient-based learning systems, using the JAX-provided automatic differentiation functionality of `jax.grad`.

    .. math::

        tba

    On spiking:

    .. math ::

        tba

    Each neuron has a membrane and synaptic time constant, :math:`\\tau_m` (`.tau_mem`) and :math:`\\tau_s` (`.tau_s`) respectively. Neurons share a common rest potential of 0, a firing threshold of 0, and a subtractive reset of -1. Neurons each have an optional bias current `.bias` (default: -1).

    On spiking, the synaptic variable :math:`I_{syn,j}` associated with each neuron is incremented by +1, and decays towards 0 with time constant :math:`\\tau_s` (`.tau_syn`).

    As output, this layer returns the synaptic current traces from the `.evolve` method. After each evolution, the attributes `.spikes_last_evolution`, `.i_rec_last_evolution` and `.v_mem_last_evolution` will be `.TimeSeries` objects containing the appropriate time series.
    """

    def __init__(
        self,
        w_recurrent: np.ndarray,
        tau_mem: FloatVector,
        tau_syn: FloatVector,
        bias: Optional[FloatVector] = -1.0,
        refractory: Optional[FloatVector] = 0.0,
        noise_std: Optional[float] = 0.0,
        dt: Optional[float] = None,
        name: Optional[str] = None,
        rng_key: Optional[int] = None,
    ):
        """
        A basic recurrent spiking neuron layer, with a JAX-implemented forward Euler solver

        :param ndarray w_recurrent:                     [N,N] Recurrent weight matrix
        :param ArrayLike[float] tau_mem:                [N,] Membrane time constants
        :param ArrayLike[float] tau_syn:                [N,] Output synaptic time constants
        :param Optional[ArrayLike[float]] bias:         [N,] Bias currents for each neuron (Default: 0)
        :param Optional[ArrayLike[float]] refractory:   [N,] Refractory period for each neuron (Default: 0)
        :param Optional[float] noise_std:               Std. dev. of white noise injected independently onto the membrane of each neuron (Default: 0)
        :param Optional[float] dt:                      Forward Euler solver time step. Default: min(tau_mem, tau_syn) / 10
        :param Optional[str] name:                      Name of this layer. Default: `None`
        :param Optional[int] rng_key:                   JAX pRNG key. Default: generate a new key
        """
        # - Ensure that weights are 2D
        w_recurrent = np.atleast_2d(w_recurrent)

        # - Transform arguments to JAX np.array
        tau_mem = np.array(tau_mem)
        tau_syn = np.array(tau_syn)
        bias = np.array(bias)
        refractory = np.array(refractory)

        if dt is None:
            dt = np.min(np.array((np.min(tau_mem), np.min(tau_syn)))) / 10.0

        # - Call super-class initialisation
        super().__init__(w_recurrent, dt, noise_std, name)

        # - Set properties
        self.tau_mem = tau_mem
        self.tau_syn = tau_syn
        self.bias = bias
        self.refractory = refractory

        # - Get compiled evolution function
        self._evolve_jit = jit(_evolve_lif_jax)

        # - Initialise "last evolution" attributes
        self.v_mem_last_evolution = None
        self.spikes_last_evolution = None

        # - Reset layer state
        self.reset_all()

        # - Seed RNG
        if rng_key is None:
            rng_key = rand.PRNGKey(onp.random.randint(0, 2 ** 63))
        _, self._rng_key = rand.split(rng_key)

    def reset_state(self):
        """
        Reset the membrane potentials, synaptic currents and refractory state for this layer
        """
        self._state = {
            'Vmem': np.zeros((self._size,)),
            'Isyn': np.zeros((self._size,)),
            'spikes': np.zeros((self._size,)),
                       }

    def evolve(
        self,
        ts_input: Optional[TSEvent] = None,
        duration: Optional[float] = None,
        num_timesteps: Optional[int] = None,
        verbose: Optional[bool] = False,
    ) -> TSContinuous:
        """
        Evolve the state of this layer given an input

        :param Optional[TSEvent] ts_input:      Input time series. Default: `None`, no stimulus is provided
        :param Optional[float] duration:        Simulation/Evolution time, in seconds. If not provided, then `num_timesteps` or the duration of `ts_input` is used to determine evolution time
        :param Optional[int] num_timesteps:     Number of evolution time steps, in units of `.dt`. If not provided, then `duration` or the duration of `ts_input` is used to determine evolution time
        :param Optional[bool]verbose:           Currently no effect, just for conformity

        :return TSContinuous:                   Output time series; the synaptic currents of each neuron
        """

        # - Prepare time base and inputs
        time_base, inps, num_timesteps = self._prepare_input_events(
            ts_input, duration, num_timesteps
        )

        # - Call raw evolution function
        time_start = self.t
        v_mem_ts, i_syn_ts, spike_raster_ts, i_rec_ts = self._evolve_raw(inps)

        # - Record membrane traces
        self.v_mem_last_evolution = TSContinuous(time_base, onp.array(v_mem_ts))

        # - Record spike raster
        spikes_ids = onp.argwhere(onp.array(spike_raster_ts))
        self.spikes_last_evolution = TSEvent(
            spikes_ids[:, 0] * self.dt + time_start,
            spikes_ids[:, 1],
            t_start=time_start,
            t_stop=self.t,
            name="Spikes " + self.name,
            num_channels=self.size,
        )

        # - Record recurrent inputs
        self.i_rec_last_evolution = TSContinuous(time_base, onp.array(i_rec_ts))

        # - Wrap spiking outputs as time series
        return TSEvent(time_base, onp.array(spike_raster_ts))

    def _evolve_raw(
        self, sp_input_ts: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Raw evolution over an input array

        :param ndarray sp_input_ts:    Input matrix [T, I]

        :return:  (v_mem_ts, i_syn_ts, spike_raster_ts)
                v_mem_ts:        (np.ndarray) Time trace of neuron membrane potentials [T, N]
                i_syn_ts:        (np.ndarray) Time trace of output synaptic currents [T, N]
                spike_raster_ts: (np.ndarray) Boolean raster [T, N]; `True` if a spike occurred in time step `t`, from neuron `n`
                i_rec_ts:        (np.ndarray) Time trace of recurrent current inputs per neuron [T, N]
        """
        # - Call compiled Euler solver to evolve reservoir
        self._state, Irec_ts, output_ts, surrogate_ts, spike_raster_ts, Vmem_ts, Isyn_ts = self._evolve_jit(
            self._state,
            0,
            self._weights,
            0,
            self._tau_mem,
            self._tau_syn,
            self._bias,
            self._noise_std,
            sp_input_ts,
            0,
            self._rng_key,
            self._dt,
        )

        # - Increment timesteps attribute
        self._timestep += sp_input_ts.shape[0] - 1

        # - Return layer activity
        return Vmem_ts, Isyn_ts, spike_raster_ts, Irec_ts

    def to_dict(self) -> dict:
        """
        Convert the configuration of this layer into a dictionary to assist in reconstruction

        :return: dict
        """
        config = super().to_dict()
        config["tau_mem"] = self.tau_mem.tolist()
        config["tau_syn"] = self.tau_syn.tolist()
        config["bias"] = self.bias.tolist()
        config["refractory"] = self.refractory.tolist()
        config["rng_key"] = self._rng_key.tolist()
        return config

    @property
    def w_recurrent(self) -> np.ndarray:
        """ (ndarray) Recurrent weight matrix [N,N] """
        return onp.array(self._weights)

    @w_recurrent.setter
    def w_recurrent(self, value: np.ndarray):
        assert np.ndim(value) == 2, "`w_recurrent` must be 2D"

        assert value.shape == (
            self._size,
            self._size,
        ), "`w_recurrent` must be [{:d}, {:d}]".format(self._size, self._size)

        self._weights = np.array(value).astype("float")

    @property
    def tau_mem(self) -> np.ndarray:
        """ (ndarray) Membrane time constant for each neuron [N,] """
        return onp.array(self._tau_mem)

    @tau_mem.setter
    def tau_mem(self, value: np.ndarray):
        # - Replicate `tau_mem` from a scalar value
        if np.size(value) == 1:
            value = np.repeat(value, self._size)

        assert (
            np.size(value) == self._size
        ), "`tau_mem` must have {:d} elements or be a scalar".format(self._size)

        self._tau_mem = np.reshape(value, self._size).astype("float")

    @property
    def tau_syn(self) -> np.ndarray:
        """ (ndarray) Output synaptic time constant for each neuron [N,] """
        return onp.array(self._tau_syn)

    @tau_syn.setter
    def tau_syn(self, value: np.ndarray):
        # - Replicate `tau_syn` from a scalar value
        if np.size(value) == 1:
            value = np.repeat(value, self._size)

        assert (
            np.size(value) == self._size
        ), "`tau_syn` must have {:d} elements or be a scalar".format(self._size)

        self._tau_syn = np.reshape(value, self._size).astype("float")

    @property
    def bias(self) -> np.ndarray:
        """ (ndarray) Bias current for each neuron [N,] """
        return onp.array(self._bias)

    @bias.setter
    def bias(self, value: np.ndarray):
        # - Replicate `bias` from a scalar value
        if np.size(value) == 1:
            value = np.repeat(value, self._size)

        assert (
            np.size(value) == self._size
        ), "`bias` must have {:d} elements or be a scalar".format(self._size)

        self._bias = np.reshape(value, self._size).astype("float")

    @property
    def dt(self) -> float:
        """ (float) Forward Euler solver time step """
        return onp.array(self._dt).item(0)

    @dt.setter
    def dt(self, value: float):
        # - Ensure dt is numerically stable
        tau_min = np.min(np.min(self._tau_mem), np.min(self._tau_syn)) / 10.0
        if value is None:
            value = tau_min

        assert value >= tau_min, "`tau` must be at least {:.2e}".format(tau_min)

        self._dt = np.array(value).astype("float")

    @property
    def output_type(self):
        """ (TSEvent) Output `.TimeSeries` class: `.TSEvent` """
        return TSEvent

    @property
    def input_type(self):
        """ (TSEvent) Input `.TimeSeries` class: `.TSEvent` """
        return TSEvent

class RecLIFCurrentInJax(RecLIFJax):
    """
    Recurrent spiking neuron layer (LIF), direct current input and spiking output. No input / output weights.

    `.RecLIFCurrentInJax` is a basic recurrent spiking neuron layer, implemented with a JAX-backed Euler solver backend. Outputs are exponential synaptic currents generated by each layer neuron; no output weighting is provided. Inputs are provided by direct current injection onto the membrane of each neuron; no input weighting is provided. The layer is therefore N inputs -> N neurons -> N outputs.

    This layer can be used to implement gradient-based learning systems, using the JAX-provided automatic differentiation functionality of `jax.grad`.

    .. math::

        tba

    On spiking:

    .. math ::

        tba

    Each neuron has a membrane and synaptic time constant, :math:`\\tau_m` (`.tau_mem`) and :math:`\\tau_s` (`.tau_s`) respectively. Neurons share a common rest potential of 0, a firing threshold of 0, and a subtractive reset of -1. Neurons each have an optional bias current `.bias` (default: -1).

    On spiking, the synaptic variable :math:`I_{syn,j}` associated with each neuron is incremented by +1, and decays towards 0 with time constant :math:`\\tau_s` (`.tau_syn`).

    As output, this layer returns the evoked spikes from the `.evolve` method. After each evolution, the attributes `.spikes_last_evolution`, `i_rec_last_evolution` and `.v_mem_last_evolution` will be `.TimeSeries` objects containing the appropriate time series.
    """

    def evolve(
        self,
        ts_input: Optional[TSContinuous] = None,
        duration: Optional[float] = None,
        num_timesteps: Optional[int] = None,
        verbose: Optional[bool] = False,
    ) -> TSEvent:
        """
        Evolve the state of this layer given an input

        :param Optional[TSContinuous] ts_input: Input time series. Default: `None`, no stimulus is provided
        :param Optional[float] duration:        Simulation/Evolution time, in seconds. If not provided, then `num_timesteps` or the duration of `ts_input` is used to determine evolution time
        :param Optional[int] num_timesteps:     Number of evolution time steps, in units of `.dt`. If not provided, then `duration` or the duration of `ts_input` is used to determine evolution time
        :param Optional[bool] verbose:          Currently no effect, just for conformity

        :return TSEvent:                   Output time series; spiking activity each neuron
        """

        # - Prepare time base and inputs
        time_base, inps, num_timesteps = self._prepare_input(
            ts_input, duration, num_timesteps
        )

        # - Call raw evolution function
        time_start = self.t
        v_mem_ts, i_syn_ts, spike_raster_ts, i_rec_ts = self._evolve_raw(inps)

        # - Record membrane traces
        self.v_mem_last_evolution = TSContinuous(time_base, onp.array(v_mem_ts))

        # - Record spike raster
        spikes_ids = onp.argwhere(onp.array(spike_raster_ts))
        self.spikes_last_evolution = TSEvent(
            spikes_ids[:, 0] * self.dt + time_start,
            spikes_ids[:, 1],
            t_start=time_start,
            t_stop=self.t,
            name="Spikes " + self.name,
            num_channels=self.size,
        )

        # - Record recurrent inputs
        self.i_rec_last_evolution = TSContinuous(time_base, onp.array(i_rec_ts))

        # - Wrap spiking outputs as time series
        return TSEvent(time_base, onp.array(spike_raster_ts))

    def _evolve_raw(
        self, I_input_ts: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Raw evolution over an input array

        :param ndarray I_input_ts:    Input matrix [T, I]

        :return:  (v_mem_ts, i_syn_ts, spike_raster_ts)
                v_mem_ts:        (np.ndarray) Time trace of neuron membrane potentials [T, N]
                i_syn_ts:        (np.ndarray) Time trace of output synaptic currents [T, N]
                spike_raster_ts: (np.ndarray) Boolean raster [T, N]; `True` if a spike occurred in time step `t`, from neuron `n`
                i_rec_ts:        (np.ndarray) Time trace of recurrent current inputs per neuron [T, N]
        """
        # - Call compiled Euler solver to evolve reservoir
        self._state, Irec_ts, output_ts, surrogate_ts, spike_raster_ts, Vmem_ts, Isyn_ts = self._evolve_jit(
            self._state,
            0,
            self._weights,
            0,
            self._tau_mem,
            self._tau_syn,
            self._bias,
            self._noise_std,
            0,
            I_input_ts,
            self._rng_key,
            self._dt,
        )

        # - Increment timesteps attribute
        self._timestep += I_input_ts.shape[0] - 1

        # - Return layer activity
        return Vmem_ts, Isyn_ts, spike_raster_ts, Irec_ts

    @property
    def output_type(self):
        """ (TSEvent) Output `.TimeSeries` class: `.TSEvent` """
        return TSEvent

    @property
    def input_type(self):
        """ (TSContinuous) Output `.TimeSeries` class: `.TSContinuous` """
        return TSContinuous
