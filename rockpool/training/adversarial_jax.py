"""
Functions to implement adversarial training approaches using Jax

See Also:
    :ref:`/tutorials/adversarial-training.ipynb` illustrates how to use the functions in this module to implement adversarial attacks on the parameters of a network during training.
"""

import numpy as np

from rockpool.nn.modules import JaxModule

import jax.tree_util as tu
import jax.random as random
from jax.lax import stop_gradient
import jax.numpy as jnp
from jax import grad, value_and_grad
from jax.lib import pytree
from jax.tree_util import tree_structure

from typing import Tuple, Callable, List, Dict, Any

RNGKey = Any

__all__ = ["pga_attack", "adversarial_loss"]


def split_and_sample_normal(key: RNGKey, shape: Tuple) -> Tuple[RNGKey, np.ndarray]:
    """
    Split an RNG key and generate random data of a given shape following a standard Gaussian distribution

    Args:
        key (RNGKey): Array of two ints. A Jax random key
        shape (tuple): The shape that the random normal data should have

    Returns:
        (RNGKey, np.ndarray): Tuple of `(key,data)`. `key` is the new key that can be used in subsequent computations and `data` is the Gaussian data
    """
    key, subkey = random.split(key)
    val = random.normal(subkey, shape=shape)
    return key, val


def eval_robustness_loss(
    theta_star: List,
    inputs: np.ndarray,
    output_theta: np.ndarray,
    net: JaxModule,
    tree_def_params: tree_structure,
    boundary_loss: Callable[[np.ndarray, np.ndarray], float],
) -> float:
    """
    Calculate the robustness loss of the adversarial attack given Theta*. This function resets the states of the network, unflattens the parameters `theta_star` and assigns them to the network `net`
    and then evaluates the network on the `inputs` using the adversarial weights. Following, the `boundary_loss` is evaluated using the outputs of the network using the original parameters `output_theta`
    and the newly generated outputs. The method returns the boundary loss. This method is used by the adversary in PGA (projected gradient ascent).

    Args:
        theta_star (List): Flattened pytree that was obtained using `jax.tree_util.tree_flatten`
        inputs (np.ndarray): Inputs that will be passed through the network
        output_theta (np.ndarray): Outputs of the network using the original weights
        net (Callable): A function (e.g. `Sequential` object) that takes an `np.ndarray` and generates another `np.ndarray`
        tree_def_params (tree_structure): Tree structure obtained by calling `jax.tree_util.tree_flatten` on `theta_star_unflattened`. Basically defining the shape of `theta`/`theta_star`
        boundary_loss (Callable): Boundary loss. Takes as input two `np.ndarray` s and returns a `float`. Example: KL divergence between softmaxed logits of the networks

    Returns:
        float: The `boundary_loss` evaluated on the outputs generated by the network using :math:`\Theta` and the outputs of the network using :math:`\Theta^*`
    """
    # - Reset the network
    net = net.reset_state()

    # - Set the network parameters to Theta*
    net = net.set_attributes(tu.tree_unflatten(tree_def_params, theta_star))

    # - Evolve the network using Theta*
    output_theta_star, _, _ = net(inputs)

    # - Return boundary loss
    return boundary_loss(output_theta, output_theta_star)


def pga_attack(
    params_flattened: List,
    net: Callable[[np.ndarray], np.ndarray],
    rand_key: np.ndarray,
    attack_steps: int,
    mismatch_level: float,
    initial_std: float,
    inputs: np.ndarray,
    output_theta: np.ndarray,
    tree_def_params: tree_structure,
    boundary_loss: Callable[[np.ndarray, np.ndarray], float],
) -> Tuple[List, Dict]:
    """
    Performs the PGA (projected gradient ascent) based attack on the parameters of the network given inputs.

    Args:
        params_flattened (List): Flattened pytree that was obtained using `jax.tree_util.tree_flatten` of the network parameters (obtained by `net.parameters()`)
        net (Callable): A function (e.g. `Sequential` object) that takes an `np.ndarray` and generates another `np.ndarray`
        rand_key (np.ndarray): Array of two `int` s. A Jax random key
        attack_steps (int): Number of PGA steps to be taken
        mismatch_level (float): Size by which the adversary can perturb the weights (:math:`\zeta`). Attack will be in :math:`[\Theta-\zeta \cdot |\Theta|,\Theta+\zeta \cdot |\Theta|]`
        initial_std (float): Initial perturbation (:math:`\zeta_{initial}`) of the parameters according to :math:`\Theta + \zeta_{initial} \cdot R \odot |\Theta| \; ; R \sim \mathcal{N}(0,\mathbf{I})`
        inputs (np.ndarray): Inputs that will be passed through the network
        output_theta (np.ndarray): Outputs of the network using the original weights
        tree_def_params (tree_structure): Tree structure obtained by calling `jax.tree_util.tree_flatten` on `theta_star_unflattened`. Basically defining the shape of `theta`/`theta_star`
        boundary_loss (Callable): Boundary loss. Takes as input two `np.ndarray` s and returns a `float`. Example: KL divergence between softmaxed logits of the networks

    Returns:
        Tuple[List,Dict]: Tuple comprising :math:`\Theta^*` in flattened form and a dictionary holding the `grads` and `losses` for every PGA iteration
    """
    # - Create verbose dict
    verbose = {"grads": [], "losses": []}

    # - Initialize Theta*
    theta_star = []
    step_size = []
    for p in params_flattened:
        rand_key, random_normal_var = split_and_sample_normal(rand_key, p.shape)
        theta_star.append(p + jnp.abs(p) * initial_std * random_normal_var)
        step_size.append((mismatch_level * jnp.abs(p)) / attack_steps)

    # - Perform the attack on Theta using initialized Theta*
    for _ in range(attack_steps):
        loss, grads_theta_star = value_and_grad(eval_robustness_loss)(
            theta_star, inputs, output_theta, net, tree_def_params, boundary_loss
        )
        verbose["losses"].append(loss)
        verbose["grads"].append(grads_theta_star)
        for idx in range(len(theta_star)):
            theta_star[idx] = theta_star[idx] + step_size[idx] * jnp.sign(
                grads_theta_star[idx]
            )

    return theta_star, verbose


def adversarial_loss(
    parameters: pytree,
    net: Callable[[np.ndarray], np.ndarray],
    inputs: np.ndarray,
    target: np.ndarray,
    training_loss: Callable[[np.ndarray, np.ndarray], float],
    boundary_loss: Callable[[np.ndarray, np.ndarray], float],
    rand_key: np.ndarray,
    noisy_forward_std: float,
    initial_std: float,
    mismatch_level: float,
    beta_robustness: float,
    attack_steps: int,
):
    """
    Implement the loss of the form :math:`\mathcal{L} = \mathcal{L}_{nat}(f(X,\Theta),y) + \\beta_{rob} \cdot \mathcal{L}_{rob}(f(X,\Theta),f(X,\mathcal{A}(\Theta)))`
    where :math:`\mathcal{A}(\Theta)` is an PGA-based adversary and :math:`\Theta` are the weights of the input that are perturbed by Gaussian noise during the forward pass.

    Args:
        parameters (pytree): Parameters of the network (obtained by e.g. `net.parameters()`)
        net (Callable): A function (e.g. `Sequential` object) that takes an `np.ndarray` and generates another `np.ndarray`
        inputs (np.ndarray): Inputs that will be passed through the network
        target (np.ndarray): Targets for the network prediction. Can be anything as long as `training_loss` can cope with the type/shape
        training_loss (Callable): Training loss. Can be anything used for training a NN (e.g. cat. cross entropy). Expects `net(inputs),target` as inputs
        boundary_loss (Callable): Boundary loss. Takes as input two `np.ndarray` s and returns a `float`. Example: KL divergence between softmaxed logits of the networks
        rand_key (np.ndarray): Array of two `int` s. A Jax random key
        noisy_forward_std (float): Float (:math:`\zeta_{forward}`) determining the amound of noise added to the parameters in the forward pass of the network. Model: :math:`\Theta = \Theta + \zeta_{forward} \cdot R \odot |\Theta| \; ; R \sim \mathcal{N}(0,\mathbf{I})`
        initial_std (float): Initial perturbation (:math:`\zeta_{initial}`) of the parameters according to :math:`\Theta + \zeta_{initial} \cdot R \odot |\Theta| \; ; R \sim \mathcal{N}(0,\mathbf{I})`
        mismatch_level (float): Size by which the adversary can perturb the weights (:math:`\zeta`). Attack will be in :math:`[\Theta-\zeta \cdot |\Theta|,\Theta+\zeta \cdot |\Theta|]`
        beta_robustness (float): Tradeoff parameter for the adversarial regularizer. Setting to `0.0` trains without adversarial loss but is much slower and should not be done.
        attack_steps (int): Number of PGA steps to be taken

    Returns:
        float: The calculated loss
    """
    # - Handle the network state â€” randomise or reset
    net = net.reset_state()

    # - Add Gaussian noise to the parameters before evaluating
    params_flattened, tree_def_params = tu.tree_flatten(parameters)

    params_gaussian_flattened = []
    for p in params_flattened:
        rand_key, random_normal_var = split_and_sample_normal(rand_key, p.shape)
        params_gaussian_flattened.append(
            p + stop_gradient(jnp.abs(p) * noisy_forward_std * random_normal_var)
        )

    # - Reshape the new parameters
    params_gaussian = tu.tree_unflatten(tree_def_params, params_gaussian_flattened)
    net = net.set_attributes(params_gaussian)

    # - Evolve the network to get the ouput
    output, _, _ = net(inputs)

    loss_n = training_loss(output, target)

    # - Get output for normal Theta
    # - Reset network state
    net = net.reset_state()

    # - Set parameters to the original parameters
    net = net.set_attributes(parameters)

    # - Get the network output using the original parameters
    output_theta, _, _ = net(inputs)

    theta_star, _ = pga_attack(
        params_flattened=params_flattened,
        net=net,
        rand_key=rand_key,
        attack_steps=attack_steps,
        mismatch_level=mismatch_level,
        initial_std=initial_std,
        inputs=inputs,
        output_theta=output_theta,
        tree_def_params=tree_def_params,
        boundary_loss=boundary_loss,
    )

    # - Compute robustness loss using final Theta*
    loss_r = eval_robustness_loss(
        theta_star, inputs, output_theta, net, tree_def_params, boundary_loss
    )

    # - Add the robustness loss as a regularizer
    return loss_n + beta_robustness * loss_r
